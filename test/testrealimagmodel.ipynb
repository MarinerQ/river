{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "promotional-vienna",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", \"Wswiglal-redir-stdio\")\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "#from .utils import PARAMETER_NAMES_ALL_PRECESSINGBNS_BILBY\n",
    "from river.models.utils import *\n",
    "from river.data.utils import *\n",
    "\n",
    "import river.data\n",
    "from river.data.datagenerator import DataGeneratorBilbyFD\n",
    "from river.data.dataset import DatasetSVDStrainFDFromSVDWFonGPU #DatasetStrainFD, #DatasetStrainFDFromPreCalSVDWF\n",
    "#import river.data.utils as datautils\n",
    "from river.data.utils import *\n",
    "\n",
    "from river.models import embedding\n",
    "from river.models.utils import *\n",
    "#from river.models.embedding.pca import project_strain_data_FDAPhi\n",
    "from river.models.embedding.conv import EmbeddingConv1D, EmbeddingConv2D\n",
    "from river.models.embedding.mlp import EmbeddingMLP1D, ResidualMLPBlock1D\n",
    "\n",
    "from glasflow.nflows.nn.nets import ResidualNet, ConvResidualNet\n",
    "from glasflow.nflows.nn.nets.resnet import ResidualBlock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "synthetic-naples",
   "metadata": {},
   "outputs": [],
   "source": [
    "ResidualBlock??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "parliamentary-herald",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import bilby \n",
    "import pycbc \n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch import nn\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "import pickle\n",
    "#from sklearn.utils.extmath import randomized_svd\n",
    "import sklearn\n",
    "from sklearn.decomposition import IncrementalPCA, randomized_svd, KernelPCA\n",
    "import sklearn.decomposition \n",
    "\n",
    "import glob\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "realistic-albert",
   "metadata": {},
   "outputs": [],
   "source": [
    "validfolder = '/home/qian.hu/mlpe/training_data/bns_50Hz1024Hz32s_lowspin_lb/valid'\n",
    "trainfolder = '/home/qian.hu/mlpe/training_data/bns_50Hz1024Hz32s_lowspin_lb/train'\n",
    "\n",
    "valid_filenames = glob.glob(f\"{validfolder}/batch*/*.h5\")\n",
    "train_filenames = glob.glob(f\"{trainfolder}/batch*/*.h5\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fallen-start",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21:48 bilby INFO    : Waveform generator initiated with\n",
      "  frequency_domain_source_model: bilby.gw.source.lal_binary_neutron_star\n",
      "  time_domain_source_model: None\n",
      "  parameter_conversion: bilby.gw.conversion.convert_to_lal_binary_black_hole_parameters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using bilby_default PSDs to generate data.\n"
     ]
    }
   ],
   "source": [
    "source_type = 'BNS'\n",
    "detector_names = ['H1', 'L1', 'V1'] \n",
    "duration = 32\n",
    "f_low = 50\n",
    "f_high = 1024\n",
    "\n",
    "f_ref = 20\n",
    "sampling_frequency = 2048\n",
    "waveform_approximant = 'IMRPhenomPv2_NRTidal'\n",
    "parameter_names = PARAMETER_NAMES_ALL_PRECESSINGBNS_BILBY\n",
    "PSD_type = 'bilby_default' #'zero_noise' bilby_default\n",
    "use_sealgw_detector = True\n",
    "\n",
    "Vhfile=\"/home/qian.hu/mlpe/river/test/outputs/Vh_50Hz1024Hz32s.pickle\"\n",
    "\n",
    "data_generator = DataGeneratorBilbyFD(source_type,\n",
    "            detector_names, \n",
    "            duration, \n",
    "            f_low, \n",
    "            f_ref, \n",
    "            sampling_frequency, \n",
    "            waveform_approximant, \n",
    "            parameter_names,\n",
    "            PSD_type=PSD_type,\n",
    "            use_sealgw_detector=use_sealgw_detector,\n",
    "            f_high=f_high)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "accredited-sharing",
   "metadata": {},
   "outputs": [],
   "source": [
    "device='cuda'\n",
    "\n",
    "dataset = DatasetSVDStrainFDFromSVDWFonGPU(valid_filenames, PARAMETER_NAMES_CONTEXT_PRECESSINGBNS_BILBY, data_generator,\n",
    "                                     Nbasis=512, Vhfile=Vhfile, device=device)\n",
    "\n",
    "batch_size = 4\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "systematic-repair",
   "metadata": {},
   "outputs": [],
   "source": [
    "theta, x = dataset[1]\n",
    "\n",
    "for theta_batch, x_batch in dataloader:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fleet-species",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 6, 512])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_batch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "naughty-creator",
   "metadata": {},
   "outputs": [],
   "source": [
    "#embd = EmbeddingConv1D(ndet=3, nout=256, num_blocks=2, use_psd = False).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "humanitarian-commons",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glasflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "progressive-layout",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NewResidualMLPBlock1D(nn.Module):\n",
    "    def __init__(self, in_features, out_features, num_blocks):\n",
    "        super().__init__()\n",
    "        self.blocks = nn.ModuleList([nn.Sequential(\n",
    "            nn.Linear(in_features if i == 0 else out_features, out_features),\n",
    "            nn.BatchNorm1d(out_features),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(out_features, out_features),\n",
    "            nn.BatchNorm1d(out_features)\n",
    "        ) for i in range(num_blocks)])\n",
    "        self.reshape = nn.Linear(in_features, out_features)  # Linear reshape for residual\n",
    "\n",
    "    def forward(self, x):\n",
    "        for i, block in enumerate(self.blocks):\n",
    "            residual = x\n",
    "            if i == 0:\n",
    "                out = block(x)\n",
    "            else:\n",
    "                out = block(out)  # Use the output from the previous block as input\n",
    "            residual = self.reshape(residual)  # Apply reshape to residual\n",
    "            out += residual\n",
    "            out = F.relu(out)\n",
    "        return out\n",
    "\n",
    "    \n",
    "\n",
    "class EmbeddingResConv1DMLP(nn.Module):\n",
    "    def __init__(self, nbasis, nout,conv_params, mlp_params, **kwargs):\n",
    "        super().__init__()\n",
    "        self.nout = nout\n",
    "        #self.nchannel = 2*ndet\n",
    "        self.nbasis = nbasis\n",
    "        \n",
    "        self.conv_params = conv_params        \n",
    "        self.mlp_params = mlp_params\n",
    "        \n",
    "        self.conv = self.make_conv_layer()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            testx = torch.randn((1, self.conv_params['in_channel'][0], self.nbasis))\n",
    "            self.mlp_params['in_features'][0] = self.conv(testx).shape[-1] * self.conv_params['out_channel'][-1]\n",
    "            print(f\"Initialized MLP in channel: {self.mlp_params['in_features'][0]}\")\n",
    "\n",
    "        \n",
    "        self.mlp = self.make_mlp_layer()\n",
    "\n",
    "    def make_conv_layer(self):\n",
    "        layers = []\n",
    "        for i in range(len(self.conv_params['in_channel'])):\n",
    "            layers.append(ResidualConvBlock1D(in_channels=self.conv_params['in_channel'][i], \n",
    "                                              out_channels=self.conv_params['out_channel'][i], \n",
    "                                              kernel_size=self.conv_params['kernel_size'][i], \n",
    "                                              stride=self.conv_params['stride'][i], \n",
    "                                              padding=self.conv_params['padding'][i], \n",
    "                                              dilation=self.conv_params['dilation'][i], \n",
    "                                              dropout=self.conv_params['dropout'][i]))\n",
    "        \n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def make_mlp_layer(self):\n",
    "        layers = []\n",
    "        for i in range(len(self.mlp_params['in_features'])):\n",
    "            layers.append(nn.Linear(self.mlp_params['in_features'][i], self.mlp_params['out_features'][i]))\n",
    "            if i!= len(self.mlp_params['in_features'])-1:\n",
    "                layers.append(nn.ReLU())\n",
    "            \n",
    "            \n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = x.flatten(start_dim=1)\n",
    "        out = self.mlp(x)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "removed-enhancement",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Object `ResidualBlock` not found.\n"
     ]
    }
   ],
   "source": [
    "ResidualBlock??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "frank-yellow",
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_params = {\n",
    "            'in_channel':  [6, 128, 128, 128, 64, 64, 64, 32, 32, 32, 16, 16, 16, 8, 8, 8, 4, 4, 4],\n",
    "            'out_channel': [128, 128, 128, 64, 64, 64, 32, 32, 32, 16, 16, 16, 8, 8, 8, 4, 4, 4, 1],\n",
    "            'kernel_size': [32, 16, 16, 16, 8, 8, 8, 8, 8, 8, 8, 4, 4, 4, 4, 2, 2, 2, 1],\n",
    "            'stride':      [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
    "            'padding':     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "            'dilation':    [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
    "            'dropout':     [0.1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "        }\n",
    "mlp_params =  {\n",
    "            'in_features': [0,],\n",
    "            'out_features': [256,],\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "accredited-director",
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_params = {\n",
    "            'in_channel': [6, ],\n",
    "            'out_channel': [1,],\n",
    "            'kernel_size': [16,],\n",
    "            'stride': [1, ],\n",
    "            'padding': [0, ],\n",
    "            'dilation': [1, ],\n",
    "            'dropout': [0.1,]\n",
    "        }\n",
    "mlp_params =  {\n",
    "            'in_features': [0,],\n",
    "            'out_features': [256,],\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "cardiac-gardening",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized MLP in channel: 372\n"
     ]
    }
   ],
   "source": [
    "embd = EmbeddingResConv1DMLP( 512, 256, conv_params, mlp_params).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "another-purse",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = embd(x_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "alleged-plaza",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 256])"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "modular-facing",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1739436"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_parameters(embd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "female-catholic",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glasflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "public-liverpool",
   "metadata": {},
   "outputs": [],
   "source": [
    "glasflow.CouplingNSF?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "signal-tobago",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.2815,  0.0368, -0.5166,  ..., -0.4752, -0.2145,  0.1430],\n",
       "        [ 0.2233, -0.1955, -0.3990,  ..., -0.1488, -0.3951, -0.1991],\n",
       "        [ 0.5356, -0.1535, -0.2723,  ..., -0.1189, -0.0980, -0.3061],\n",
       "        [-0.0269, -0.1622, -0.7113,  ...,  0.2489,  0.0599,  0.1411]],\n",
       "       device='cuda:0', grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "invalid-drove",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13926400"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(1024**2 + 512**2  + 256**2 + 128**2) * 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nearby-narrow",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "criminal-government",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 6, 512])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_batch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "sonic-pitch",
   "metadata": {},
   "outputs": [],
   "source": [
    "tt = torch.cat((x_batch,x_batch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "instrumental-curtis",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 6, 512])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tt.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "id": "simplified-monster",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class ResidualConvBlock2D(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=1, stride=1, padding=0, dilation=1):\n",
    "        super().__init__()\n",
    "        #self.padding=padding\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=kernel_size, stride=stride, padding=padding, dilation=dilation)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=1, padding=0, dilation=1)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "        \n",
    "        \n",
    "        if in_channels != out_channels or kernel_size!=1 or stride!=1 or padding!=0 or dilation!=1:\n",
    "            self.downsample = nn.Conv2d(in_channels, out_channels, kernel_size, stride=stride, padding=padding, dilation=dilation) \n",
    "        else:\n",
    "            self.downsample = nn.Identity()\n",
    "\n",
    "        '''\n",
    "        self.downsample = nn.Sequential()\n",
    "        if stride != 1 or in_channels != out_channels:\n",
    "            self.downsample = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels, kernel_size=1),\n",
    "                nn.BatchNorm2d(out_channels)\n",
    "            )\n",
    "        '''\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = F.relu(out)\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out += self.downsample(residual)\n",
    "        out = F.relu(out)\n",
    "        return out\n",
    "\n",
    "class MyEmbeddingConv2D(nn.Module):\n",
    "    def __init__(self, nout, **kwargs):\n",
    "        super().__init__()\n",
    "\n",
    "        self.nout = nout\n",
    "        self.layers = nn.ModuleList([self.make_layer()])\n",
    "        self.linear = nn.Linear(3, self.nout)\n",
    "\n",
    "    def make_layer(self):\n",
    "        layers = []\n",
    "        layers.append(ResidualConvBlock2D(in_channels=1, out_channels=8,\n",
    "                                          kernel_size=(3,2), stride=(1,1), dilation=(1,1), padding = (2,0)))\n",
    "        layers.append(ResidualConvBlock2D(in_channels=8, out_channels=16,\n",
    "                                          kernel_size=(3,4), stride=(1,1), dilation=(2,1), padding = (2,0)))\n",
    "        layers.append(ResidualConvBlock2D(in_channels=16, out_channels=16,\n",
    "                                          kernel_size=(3,2), stride=(1,1), dilation=(2,1), padding = (2,0)))\n",
    "        layers.append(ResidualConvBlock2D(in_channels=16, out_channels=3,\n",
    "                                          kernel_size=(3,2), stride=(1,1), dilation=(1,1), padding = (2,0)))\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x : [batch_size, channel (det_123, amp/phase) = 2*ndet, length (number of samples)]\n",
    "        # bs,_,_,_  = x.shape\n",
    "        x = x.unsqueeze(1)\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        x = F.avg_pool2d(x, x.size()[2:])\n",
    "        x = x.view(x.size(0), -1)\n",
    "        output = self.linear(x)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "efficient-kazakhstan",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = x_batch.unsqueeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "unavailable-rover",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 1, 6, 512])"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "id": "supported-midwest",
   "metadata": {},
   "outputs": [],
   "source": [
    "embd = MyEmbeddingConv2D(nout=256).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "id": "removable-endorsement",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8726"
      ]
     },
     "execution_count": 245,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_parameters(embd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "entitled-sequence",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "integrated-olive",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.nn.Conv2d?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "exposed-cross",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer1 = torch.nn.Conv2d(1, 8, kernel_size=(3,2), stride=(1,1), dilation=(1,1), padding = (2,0)).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "based-celtic",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer1 = ResidualConvBlock2D(1, 8, kernel_size=(3,2), stride=(4,2), dilation=(3,4), padding = (3,2)).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pregnant-pillow",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "id": "compound-shaft",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = embd(x_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cloudy-wrong",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "id": "african-education",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 256])"
      ]
     },
     "execution_count": 247,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "collectible-philosophy",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glasflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "blind-lightning",
   "metadata": {},
   "outputs": [],
   "source": [
    "glasflow.nflows.nn.nets.ConvResidualNet??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "negative-masters",
   "metadata": {},
   "outputs": [],
   "source": [
    "nn.Conv2d?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "hazardous-immunology",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (255) must match the size of tensor b (512) at non-singleton dimension 3",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-138-51c2452729a4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcondition\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0membd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_batch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.conda/envs/myigwn-py39/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-134-5596c64891e8>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     55\u001b[0m         \u001b[0;31m# bs,_,_,_  = x.shape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mavg_pool2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/myigwn-py39/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/myigwn-py39/lib/python3.9/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    202\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 204\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    205\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/myigwn-py39/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-134-5596c64891e8>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbn2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownsample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresidual\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (255) must match the size of tensor b (512) at non-singleton dimension 3"
     ]
    }
   ],
   "source": [
    "condition = embd(x_batch.unsqueeze(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "disciplinary-apparatus",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 256])"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "condition.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "prescription-baseball",
   "metadata": {},
   "outputs": [],
   "source": [
    "F.avg_pool2d?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "imperial-firewall",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "\"avg_pool2d\" not implemented for 'ComplexFloat'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-144-d20feb3e905f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0maa\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1j\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m6\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m7\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcomplex64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mavg_pool1d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maa\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkernel_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstride\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m: \"avg_pool2d\" not implemented for 'ComplexFloat'"
     ]
    }
   ],
   "source": [
    "aa = torch.tensor([[[1+1j, 2, 3, 4, 5, 6, 7]]], dtype=torch.complex64)\n",
    "F.avg_pool1d(aa, kernel_size=3, stride=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "golden-supervisor",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myigwn-py39",
   "language": "python",
   "name": "myigwn-py39"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
