{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "considered-teaching",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<frozen importlib._bootstrap>:228: RuntimeWarning: scipy._lib.messagestream.MessageStream size changed, may indicate binary incompatibility. Expected 56 from C header, got 64 from PyObject\n",
      "/home/qian.hu/.local/lib/python3.9/site-packages/glue/ligolw/lsctables.py:50: UserWarning: Wswiglal-redir-stdio:\n",
      "\n",
      "SWIGLAL standard output/error redirection is enabled in IPython.\n",
      "This may lead to performance penalties. To disable locally, use:\n",
      "\n",
      "with lal.no_swig_redirect_standard_output_error():\n",
      "    ...\n",
      "\n",
      "To disable globally, use:\n",
      "\n",
      "lal.swig_redirect_standard_output_error(True)\n",
      "\n",
      "Note however that this will likely lead to error messages from\n",
      "LAL functions being either misdirected or lost when called from\n",
      "Jupyter notebooks.\n",
      "\n",
      "To suppress this warning, use:\n",
      "\n",
      "import warnings\n",
      "warnings.filterwarnings(\"ignore\", \"Wswiglal-redir-stdio\")\n",
      "import lal\n",
      "\n",
      "  import lal\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import bilby \n",
    "#import pycbc \n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "import glob \n",
    "\n",
    "#import zuko\n",
    "from glasflow import RealNVP, CouplingNSF\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch import nn\n",
    "\n",
    "import river.data\n",
    "from river.data.datagenerator import DataGeneratorBilbyFD\n",
    "from river.data.dataset import DatasetSVDStrainFDFromSVDWFonGPU, DatasetSVDStrainFDFromSVDWFonGPUBatch\n",
    "#import river.data.utils as datautils\n",
    "from river.data.utils import *\n",
    "\n",
    "from river.models import embedding\n",
    "from river.models.utils import *\n",
    "from river.models.embedding.conv import EmbeddingConv1D, EmbeddingConv2D\n",
    "from river.models.embedding.mlp import EmbeddingMLP1D\n",
    "from river.models.inference.cnf import GlasNSFConv1DRes, GlasNSFConv1D, GlasNSFTest, GlasflowEmbdding\n",
    "\n",
    "import logging\n",
    "import sys\n",
    "import os\n",
    "import json\n",
    "from copy import deepcopy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "painted-container",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "02:10 bilby INFO    : Waveform generator initiated with\n",
      "  frequency_domain_source_model: bilby.gw.source.lal_binary_neutron_star\n",
      "  time_domain_source_model: None\n",
      "  parameter_conversion: bilby.gw.conversion.convert_to_lal_binary_black_hole_parameters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using bilby_default PSDs to generate data.\n"
     ]
    }
   ],
   "source": [
    "config_path = 'test_train_output'\n",
    "with open(f\"{config_path}/config.json\", 'r') as f:\n",
    "    config = json.load(f)\n",
    "\n",
    "config_datagenerator = config['data_generator_parameters']\n",
    "config_training = config['training_parameters']\n",
    "config_model = config['model_parameters']\n",
    "config_precaldata = config['precaldata_parameters']\n",
    "\n",
    "\n",
    "\n",
    "# Set up logger\n",
    "PID = os.getpid()\n",
    "device='cuda:1'\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "formatter = logging.Formatter('%(asctime)s | %(levelname)s | %(message)s')\n",
    "\n",
    "stdout_handler = logging.StreamHandler(sys.stdout)\n",
    "stdout_handler.setLevel(logging.DEBUG)\n",
    "stdout_handler.setFormatter(formatter)\n",
    "\n",
    "ckpt_dir = config['ckpt_dir']\n",
    "if not os.path.exists(ckpt_dir):\n",
    "    os.mkdir(ckpt_dir)\n",
    "    logger.warning(f\"{ckpt_dir} does not exist. Made dir {ckpt_dir}.\")\n",
    "\n",
    "logfilename = f\"{ckpt_dir}/logs.log\"\n",
    "file_handler = logging.FileHandler(logfilename)\n",
    "file_handler.setLevel(logging.DEBUG)\n",
    "file_handler.setFormatter(formatter)\n",
    "logger.addHandler(file_handler)\n",
    "ckpt_path = f'{ckpt_dir}/checkpoint.pickle'\n",
    "\n",
    "logger.info(f'PID={PID}.')\n",
    "logger.info(f'Output path: {ckpt_dir}')\n",
    "\n",
    "detector_names = config_datagenerator['detector_names']\n",
    "\n",
    "\n",
    "\n",
    "logger.info(f'Loading precalculated data.')\n",
    "train_filenames = glob.glob(f\"{config_precaldata['train']['folder']}/batch*/*.h5\")[:2]\n",
    "valid_filenames = glob.glob(f\"{config_precaldata['valid']['folder']}/batch*/*.h5\")\n",
    "#logger.info(f'{len(train_precaldata_filelist)}, {len(valid_precaldata_filelist)}')\n",
    "\n",
    "data_generator = DataGeneratorBilbyFD(**config_datagenerator)\n",
    "\n",
    "Vhfile = config_model['Vhfile']\n",
    "Nbasis = config_model['Nbasis']\n",
    "batch_size_train = config_training['batch_size_train']\n",
    "minibatch_size_train = config_training['minibatch_size_train']\n",
    "batch_size_valid = config_training['batch_size_valid']\n",
    "\n",
    "\n",
    "dataset_train = DatasetSVDStrainFDFromSVDWFonGPUBatch(train_filenames, PARAMETER_NAMES_CONTEXT_PRECESSINGBNS_BILBY, data_generator,\n",
    "                                 Nbasis=Nbasis, Vhfile=Vhfile, device=device, minibatch_size=minibatch_size_train, \n",
    "                                                      fix_extrinsic=True, shuffle=False, add_noise=False)\n",
    "dataset_valid = DatasetSVDStrainFDFromSVDWFonGPU(valid_filenames, PARAMETER_NAMES_CONTEXT_PRECESSINGBNS_BILBY, data_generator,\n",
    "                                 Nbasis=Nbasis, Vhfile=Vhfile, device=device, fix_extrinsic=True, shuffle=False, add_noise=False)\n",
    "\n",
    "\n",
    "\n",
    "Nsample = len(dataset_train)*minibatch_size_train\n",
    "Nvalid = len(dataset_valid)\n",
    "logger.info(f'Nsample: {Nsample}, Nvalid: {Nvalid}.')\n",
    "logger.info(f'batch_size_train: {batch_size_train}, batch_size_valid: {batch_size_valid}')\n",
    "\n",
    "batch_size_train = 4096\n",
    "train_loader = DataLoader(dataset_train, batch_size=batch_size_train // minibatch_size_train, shuffle=False)\n",
    "valid_loader = DataLoader(dataset_valid, batch_size=batch_size_valid, shuffle=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "graduate-merchandise",
   "metadata": {},
   "outputs": [],
   "source": [
    "config_dict = config.copy()\n",
    "config_dict['model_parameters']['embedding']['model'] = 'EmbeddingResConv1DMLP'\n",
    "NCOND = 64\n",
    "config_dict['model_parameters']['embedding']['nout'] = NCOND\n",
    "#config_dict['model_parameters']['embedding']['ndet'] = 3\n",
    "config_dict['model_parameters']['embedding']['nbasis'] = config_dict['model_parameters']['Nbasis']\n",
    "config_dict['model_parameters']['embedding']['conv_params'] = {\n",
    "        'in_channel':  [6,  3, ],\n",
    "        'out_channel': [3, 1],\n",
    "        'kernel_size': [16, 16, 16, 8, 8, 8, 8, 8, 8, 8, 4, 4, 4, 4, 2, 2, 2, 1],\n",
    "        'stride':      [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
    "        'padding':     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "        'dilation':    [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
    "        'dropout':     [0.1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "    }\n",
    "config_dict['model_parameters']['embedding']['mlp_params'] = {\n",
    "        'in_features': [0,],\n",
    "        'out_features': [NCOND,],\n",
    "    }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "associate-memorial",
   "metadata": {},
   "outputs": [],
   "source": [
    "config_dict['model_parameters']['flow'] = {}\n",
    "\n",
    "config_dict['model_parameters']['flow']['model'] = 'CouplingNSF'\n",
    "config_dict['model_parameters']['flow']['n_inputs'] = 17 \n",
    "config_dict['model_parameters']['flow']['n_transforms'] = 3\n",
    "config_dict['model_parameters']['flow']['n_conditional_inputs'] = NCOND\n",
    "config_dict['model_parameters']['flow']['n_neurons'] = 6  # 32 by default\n",
    "config_dict['model_parameters']['flow']['batch_norm_between_transforms'] = True\n",
    "config_dict['model_parameters']['flow']['batch_norm_within_blocks'] = False\n",
    "config_dict['model_parameters']['flow']['n_blocks_per_transform'] = 2  # 2 by default, 5\n",
    "config_dict['model_parameters']['flow']['num_bins'] = 4  # 4 by default, 8\n",
    "config_dict['model_parameters']['flow']['tail_bound'] = 1 # 5 by default, 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "suffering-chain",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized MLP in channel: 482\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#model = GlasNSFConv1DRes(config).to(device)\n",
    "#model = GlasNSFConv1D(config).to(device)\n",
    "model = GlasflowEmbdding(config_dict).to(device)\n",
    "\n",
    "\n",
    "lr = config_training['lr']\n",
    "gamma = config_training['gamma']\n",
    "weight_decay = config_training['weight_decay']\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "logger.info(f'Initial learning rate: {lr}')\n",
    "logger.info(f'Gamma: {gamma}')\n",
    "\n",
    "max_epoch = config_training['max_epoch']\n",
    "#epoches_pretrain = config_training['epoches_pretrain']\n",
    "epoches_save_loss = config_training['epoches_save_loss']\n",
    "epoches_adjust_lr = config_training['epoches_adjust_lr']\n",
    "epoches_adjust_lr_again = config_training['epoches_adjust_lr_again']\n",
    "#load_from_previous_train = 1\n",
    "\n",
    "load_from_previous_train = False\n",
    "if load_from_previous_train:\n",
    "    checkpoint = torch.load(ckpt_path)\n",
    "\n",
    "    best_epoch = checkpoint['epoch']\n",
    "    start_epoch = best_epoch + 1\n",
    "    lr_updated_epoch = start_epoch\n",
    "    model.load_state_dict(checkpoint['model_state_dict']) \n",
    "\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "\n",
    "\n",
    "    train_losses = checkpoint['train_losses']\n",
    "    valid_losses = checkpoint['valid_losses']\n",
    "\n",
    "\n",
    "    logger.info(f'Loaded states from {ckpt_path}, epoch={start_epoch}.')\n",
    "else:\n",
    "    best_epoch = 0\n",
    "    train_losses = []\n",
    "    valid_losses = []\n",
    "\n",
    "    start_epoch = 0\n",
    "    lr_updated_epoch = start_epoch\n",
    "\n",
    "npara_flow = count_parameters(model.flow)\n",
    "#npara_embd_proj = count_parameters(model.embedding)\n",
    "#npara_embd_res = count_parameters(model.resnet)\n",
    "npara_total = count_parameters(model)\n",
    "#logger.info(f'Learnable parameters: flow: {npara_flow}, embedding_PCA: {npara_embd_proj}, ResNet: {npara_embd_res}. Total: {npara_total}. ')\n",
    "#logger.info(f'Learnable parameters: flow: {npara_flow}, embedding_PCA: {npara_embd_proj}. Total: {npara_total}. ')\n",
    "logger.info(f'Learnable parameters: flow: {npara_flow}, total: {npara_total}. ')\n",
    "\n",
    "###\n",
    "#for g in optimizer.param_groups:\n",
    "#    g['lr'] = 1e-5\n",
    "#    logger.info(f'Set lr to 1e-5.')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "equivalent-responsibility",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "advised-indianapolis",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Learnable parameters: flow: 6197, total: 37819. '"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f'Learnable parameters: flow: {npara_flow}, total: {npara_total}. '"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "located-excuse",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start_epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "fluid-snapshot",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mytrain_GlasNSFWarpper(model, optimizer, dataloader, detector_names=None, ipca_gen=None, device='cpu',downsample_rate=1, minibatch_size=0):\n",
    "    model.train()\n",
    "    loss_list = []\n",
    "    for theta, x in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        theta = theta.to(device)\n",
    "        x = x.to(device)\n",
    "        \n",
    "        if minibatch_size>0:\n",
    "            # x: [bs, minibatch_size, nchannel, nbasis]\n",
    "            # theta: [bs, minibatch_size, npara]\n",
    "            bs = x.shape[0]\n",
    "            nbasis = x.shape[-1]\n",
    "            nchannel = x.shape[-2]\n",
    "            npara = theta.shape[-1]\n",
    "            theta = theta.view(bs*minibatch_size, npara)\n",
    "            x = x.view(bs*minibatch_size, nchannel, nbasis)\n",
    "        loss = -model.log_prob(theta, x).mean()\n",
    "        print('train loss ', loss)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        loss_list.append(loss.detach())\n",
    "\n",
    "    mean_loss = torch.stack(loss_list).mean().item() # mean(list of mean losses of each batch)\n",
    "    std_loss = torch.stack(loss_list).std().item()\n",
    "    return mean_loss, std_loss\n",
    "\n",
    "def myeval_GlasNSFWarpper(model, dataloader, detector_names=None, ipca_gen=None, device='cpu',downsample_rate=1):\n",
    "    model.eval()\n",
    "    loss_list = []\n",
    "    with torch.no_grad():\n",
    "        for theta, x in dataloader:\n",
    "            theta = theta.to(device)\n",
    "            x = x.to(device)\n",
    "            loss = -model.log_prob(theta, x).mean()\n",
    "            print('valid loss ', loss)\n",
    "            loss_list.append(loss.detach())\n",
    "\n",
    "    mean_loss = torch.stack(loss_list).mean().item()\n",
    "    std_loss = torch.stack(loss_list).std().item()\n",
    "    return mean_loss, std_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "hazardous-gross",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss  tensor(-1.8704, device='cuda:1', grad_fn=<NegBackward0>)\n",
      "train loss  tensor(-3.5586, device='cuda:1', grad_fn=<NegBackward0>)\n",
      "train loss  tensor(-5.5375, device='cuda:1', grad_fn=<NegBackward0>)\n",
      "train loss  tensor(-7.9637, device='cuda:1', grad_fn=<NegBackward0>)\n",
      "train loss  tensor(-10.2041, device='cuda:1', grad_fn=<NegBackward0>)\n",
      "train loss  tensor(-12.0127, device='cuda:1', grad_fn=<NegBackward0>)\n",
      "train loss  tensor(-12.9838, device='cuda:1', grad_fn=<NegBackward0>)\n",
      "train loss  tensor(-13.4175, device='cuda:1', grad_fn=<NegBackward0>)\n",
      "train loss  tensor(-13.8539, device='cuda:1', grad_fn=<NegBackward0>)\n",
      "train loss  tensor(-14.2418, device='cuda:1', grad_fn=<NegBackward0>)\n",
      "train loss  tensor(-14.7444, device='cuda:1', grad_fn=<NegBackward0>)\n",
      "train loss  tensor(-15.0322, device='cuda:1', grad_fn=<NegBackward0>)\n",
      "train loss  tensor(-15.1203, device='cuda:1', grad_fn=<NegBackward0>)\n",
      "train loss  tensor(-15.2579, device='cuda:1', grad_fn=<NegBackward0>)\n",
      "train loss  tensor(-15.7487, device='cuda:1', grad_fn=<NegBackward0>)\n",
      "train loss  tensor(-15.9783, device='cuda:1', grad_fn=<NegBackward0>)\n",
      "train loss  tensor(-16.6146, device='cuda:1', grad_fn=<NegBackward0>)\n",
      "train loss  tensor(-17.0471, device='cuda:1', grad_fn=<NegBackward0>)\n",
      "train loss  tensor(-17.2494, device='cuda:1', grad_fn=<NegBackward0>)\n",
      "train loss  tensor(-17.5181, device='cuda:1', grad_fn=<NegBackward0>)\n",
      "train loss  tensor(-17.9418, device='cuda:1', grad_fn=<NegBackward0>)\n",
      "train loss  tensor(-18.3620, device='cuda:1', grad_fn=<NegBackward0>)\n",
      "train loss  tensor(-18.3619, device='cuda:1', grad_fn=<NegBackward0>)\n",
      "train loss  tensor(-18.7488, device='cuda:1', grad_fn=<NegBackward0>)\n",
      "train loss  tensor(-19.3690, device='cuda:1', grad_fn=<NegBackward0>)\n",
      "train loss  tensor(-19.7221, device='cuda:1', grad_fn=<NegBackward0>)\n",
      "train loss  tensor(-20.0771, device='cuda:1', grad_fn=<NegBackward0>)\n",
      "train loss  tensor(-20.9034, device='cuda:1', grad_fn=<NegBackward0>)\n",
      "train loss  tensor(-21.8818, device='cuda:1', grad_fn=<NegBackward0>)\n",
      "train loss  tensor(-22.1927, device='cuda:1', grad_fn=<NegBackward0>)\n",
      "train loss  tensor(-23.0518, device='cuda:1', grad_fn=<NegBackward0>)\n",
      "train loss  tensor(-23.2177, device='cuda:1', grad_fn=<NegBackward0>)\n",
      "train loss  tensor(-24.1416, device='cuda:1', grad_fn=<NegBackward0>)\n",
      "train loss  tensor(-24.9103, device='cuda:1', grad_fn=<NegBackward0>)\n",
      "train loss  tensor(-25.9113, device='cuda:1', grad_fn=<NegBackward0>)\n",
      "train loss  tensor(-25.8013, device='cuda:1', grad_fn=<NegBackward0>)\n",
      "train loss  tensor(-26.7867, device='cuda:1', grad_fn=<NegBackward0>)\n",
      "train loss  tensor(-27.0091, device='cuda:1', grad_fn=<NegBackward0>)\n",
      "train loss  tensor(-27.1497, device='cuda:1', grad_fn=<NegBackward0>)\n",
      "train loss  tensor(-27.3799, device='cuda:1', grad_fn=<NegBackward0>)\n",
      "train loss  tensor(-28.0475, device='cuda:1', grad_fn=<NegBackward0>)\n",
      "train loss  tensor(-28.1591, device='cuda:1', grad_fn=<NegBackward0>)\n",
      "train loss  tensor(-28.8296, device='cuda:1', grad_fn=<NegBackward0>)\n",
      "train loss  tensor(-29.2101, device='cuda:1', grad_fn=<NegBackward0>)\n",
      "train loss  tensor(-29.6984, device='cuda:1', grad_fn=<NegBackward0>)\n",
      "train loss  tensor(-30.2589, device='cuda:1', grad_fn=<NegBackward0>)\n",
      "train loss  tensor(-30.6657, device='cuda:1', grad_fn=<NegBackward0>)\n",
      "train loss  tensor(-31.3799, device='cuda:1', grad_fn=<NegBackward0>)\n",
      "train loss  tensor(-32.0953, device='cuda:1', grad_fn=<NegBackward0>)\n",
      "train loss  tensor(-32.5997, device='cuda:1', grad_fn=<NegBackward0>)\n",
      "train loss  tensor(-33.3508, device='cuda:1', grad_fn=<NegBackward0>)\n",
      "train loss  tensor(-34.0245, device='cuda:1', grad_fn=<NegBackward0>)\n",
      "train loss  tensor(-34.6445, device='cuda:1', grad_fn=<NegBackward0>)\n",
      "train loss  tensor(-35.4185, device='cuda:1', grad_fn=<NegBackward0>)\n",
      "train loss  tensor(-35.7919, device='cuda:1', grad_fn=<NegBackward0>)\n",
      "train loss  tensor(-36.0667, device='cuda:1', grad_fn=<NegBackward0>)\n",
      "train loss  tensor(-36.5622, device='cuda:1', grad_fn=<NegBackward0>)\n",
      "train loss  tensor(-37.3657, device='cuda:1', grad_fn=<NegBackward0>)\n",
      "train loss  tensor(-37.5631, device='cuda:1', grad_fn=<NegBackward0>)\n",
      "train loss  tensor(-38.0262, device='cuda:1', grad_fn=<NegBackward0>)\n",
      "train loss  tensor(-38.2805, device='cuda:1', grad_fn=<NegBackward0>)\n",
      "train loss  tensor(-38.4779, device='cuda:1', grad_fn=<NegBackward0>)\n",
      "train loss  tensor(-39.0505, device='cuda:1', grad_fn=<NegBackward0>)\n",
      "train loss  tensor(-38.7013, device='cuda:1', grad_fn=<NegBackward0>)\n",
      "valid loss  tensor(7.1237e+11, device='cuda:1')\n",
      "valid loss  tensor(7.1237e+11, device='cuda:1')\n",
      "recal train loss  tensor(-39.1560, device='cuda:1')\n",
      "recal valid loss  tensor(-60.4201, device='cuda:1')\n",
      "train loss  tensor(-39.1127, device='cuda:1', grad_fn=<NegBackward0>)\n",
      "train loss  tensor(-39.6031, device='cuda:1', grad_fn=<NegBackward0>)\n",
      "train loss  tensor(-39.5760, device='cuda:1', grad_fn=<NegBackward0>)\n",
      "train loss  tensor(-39.8085, device='cuda:1', grad_fn=<NegBackward0>)\n",
      "train loss  tensor(-40.4336, device='cuda:1', grad_fn=<NegBackward0>)\n",
      "train loss  tensor(-40.5501, device='cuda:1', grad_fn=<NegBackward0>)\n",
      "train loss  tensor(-40.7118, device='cuda:1', grad_fn=<NegBackward0>)\n",
      "train loss  tensor(-40.9149, device='cuda:1', grad_fn=<NegBackward0>)\n",
      "train loss  tensor(-41.2159, device='cuda:1', grad_fn=<NegBackward0>)\n",
      "train loss  tensor(-41.0897, device='cuda:1', grad_fn=<NegBackward0>)\n",
      "train loss  tensor(-41.2982, device='cuda:1', grad_fn=<NegBackward0>)\n",
      "train loss  tensor(-41.7839, device='cuda:1', grad_fn=<NegBackward0>)\n",
      "train loss  tensor(-41.9612, device='cuda:1', grad_fn=<NegBackward0>)\n",
      "train loss  tensor(-42.2893, device='cuda:1', grad_fn=<NegBackward0>)\n",
      "train loss  tensor(-42.0090, device='cuda:1', grad_fn=<NegBackward0>)\n",
      "train loss  tensor(-42.2798, device='cuda:1', grad_fn=<NegBackward0>)\n",
      "train loss  tensor(-42.1558, device='cuda:1', grad_fn=<NegBackward0>)\n",
      "train loss  tensor(-43.0994, device='cuda:1', grad_fn=<NegBackward0>)\n",
      "train loss  tensor(-42.7987, device='cuda:1', grad_fn=<NegBackward0>)\n",
      "train loss  tensor(-42.6780, device='cuda:1', grad_fn=<NegBackward0>)\n",
      "train loss  tensor(-43.2208, device='cuda:1', grad_fn=<NegBackward0>)\n",
      "train loss  tensor(-43.6729, device='cuda:1', grad_fn=<NegBackward0>)\n",
      "train loss  tensor(-43.1115, device='cuda:1', grad_fn=<NegBackward0>)\n",
      "train loss  tensor(-43.3378, device='cuda:1', grad_fn=<NegBackward0>)\n",
      "train loss  tensor(-43.9437, device='cuda:1', grad_fn=<NegBackward0>)\n",
      "train loss  tensor(-44.2839, device='cuda:1', grad_fn=<NegBackward0>)\n",
      "train loss  tensor(-44.4997, device='cuda:1', grad_fn=<NegBackward0>)\n",
      "train loss  tensor(-44.6224, device='cuda:1', grad_fn=<NegBackward0>)\n",
      "train loss  tensor(-44.5504, device='cuda:1', grad_fn=<NegBackward0>)\n",
      "train loss  tensor(-44.9983, device='cuda:1', grad_fn=<NegBackward0>)\n",
      "train loss  tensor(-44.8341, device='cuda:1', grad_fn=<NegBackward0>)\n",
      "train loss  tensor(-45.2110, device='cuda:1', grad_fn=<NegBackward0>)\n",
      "train loss  tensor(-44.9965, device='cuda:1', grad_fn=<NegBackward0>)\n",
      "train loss  tensor(-45.2247, device='cuda:1', grad_fn=<NegBackward0>)\n",
      "train loss  tensor(-45.4364, device='cuda:1', grad_fn=<NegBackward0>)\n",
      "train loss  tensor(-45.7195, device='cuda:1', grad_fn=<NegBackward0>)\n",
      "train loss  tensor(-46.1856, device='cuda:1', grad_fn=<NegBackward0>)\n",
      "train loss  tensor(-46.2945, device='cuda:1', grad_fn=<NegBackward0>)\n",
      "train loss  tensor(-46.8048, device='cuda:1', grad_fn=<NegBackward0>)\n",
      "train loss  tensor(-46.4393, device='cuda:1', grad_fn=<NegBackward0>)\n",
      "train loss  tensor(-47.2966, device='cuda:1', grad_fn=<NegBackward0>)\n",
      "train loss  tensor(-46.8456, device='cuda:1', grad_fn=<NegBackward0>)\n",
      "train loss  tensor(-47.3346, device='cuda:1', grad_fn=<NegBackward0>)\n",
      "train loss  tensor(-47.2537, device='cuda:1', grad_fn=<NegBackward0>)\n",
      "train loss  tensor(-47.5190, device='cuda:1', grad_fn=<NegBackward0>)\n",
      "train loss  tensor(-47.5255, device='cuda:1', grad_fn=<NegBackward0>)\n",
      "train loss  tensor(-47.6786, device='cuda:1', grad_fn=<NegBackward0>)\n",
      "train loss  tensor(-47.9413, device='cuda:1', grad_fn=<NegBackward0>)\n",
      "train loss  tensor(-47.6404, device='cuda:1', grad_fn=<NegBackward0>)\n",
      "train loss  tensor(-47.7431, device='cuda:1', grad_fn=<NegBackward0>)\n",
      "train loss  tensor(-47.9879, device='cuda:1', grad_fn=<NegBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss  tensor(-48.5215, device='cuda:1', grad_fn=<NegBackward0>)\n",
      "train loss  tensor(-48.0518, device='cuda:1', grad_fn=<NegBackward0>)\n",
      "train loss  tensor(-48.5588, device='cuda:1', grad_fn=<NegBackward0>)\n",
      "train loss  tensor(-48.4366, device='cuda:1', grad_fn=<NegBackward0>)\n",
      "train loss  tensor(-48.7357, device='cuda:1', grad_fn=<NegBackward0>)\n",
      "train loss  tensor(-48.3036, device='cuda:1', grad_fn=<NegBackward0>)\n",
      "train loss  tensor(-48.8190, device='cuda:1', grad_fn=<NegBackward0>)\n",
      "train loss  tensor(-49.3209, device='cuda:1', grad_fn=<NegBackward0>)\n",
      "train loss  tensor(-48.9690, device='cuda:1', grad_fn=<NegBackward0>)\n",
      "train loss  tensor(-49.5478, device='cuda:1', grad_fn=<NegBackward0>)\n",
      "train loss  tensor(-49.1854, device='cuda:1', grad_fn=<NegBackward0>)\n",
      "train loss  tensor(-49.4106, device='cuda:1', grad_fn=<NegBackward0>)\n",
      "train loss  tensor(-49.4097, device='cuda:1', grad_fn=<NegBackward0>)\n",
      "valid loss  tensor(10425445., device='cuda:1')\n",
      "valid loss  tensor(10425443., device='cuda:1')\n",
      "recal train loss  tensor(-49.8176, device='cuda:1')\n",
      "recal valid loss  tensor(-69.6011, device='cuda:1')\n"
     ]
    }
   ],
   "source": [
    "logger.info(f'Training started, device:{device}. ')\n",
    "\n",
    "max_epoch = 5\n",
    "#for epoch in range(start_epoch, max_epoch):    \n",
    "for epoch in range(0, 2):   \n",
    "    train_loss, train_loss_std = mytrain_GlasNSFWarpper(model, optimizer, train_loader, device=device, minibatch_size=minibatch_size_train)\n",
    "    valid_loss, valid_loss_std = myeval_GlasNSFWarpper(model, valid_loader, device=device)\n",
    "    \n",
    "    #\n",
    "    model.train()\n",
    "    with torch.no_grad():\n",
    "        for theta, x in train_loader:\n",
    "\n",
    "            theta = theta.to(device)\n",
    "            x = x.to(device)\n",
    "            bs = x.shape[0]\n",
    "            nbasis = x.shape[-1]\n",
    "            nchannel = x.shape[-2]\n",
    "            npara = theta.shape[-1]\n",
    "            theta = theta.view(bs*minibatch_size_train, npara)\n",
    "            x = x.view(bs*minibatch_size_train, nchannel, nbasis)\n",
    "\n",
    "            loss = -model.log_prob(theta, x).mean()\n",
    "            print('recal train loss ', loss)\n",
    "\n",
    "            break\n",
    "        \n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for theta, x in valid_loader:\n",
    "            \n",
    "            theta = theta[0:2].to(device)\n",
    "            x = x[0:2].to(device)\n",
    "            loss = -model.log_prob(theta, x).mean()\n",
    "            print('recal valid loss ', loss)\n",
    "            \n",
    "            break\n",
    "            \n",
    "    train_losses.append(train_loss)\n",
    "    valid_losses.append(valid_loss)\n",
    "\n",
    "    logger.info(f'epoch {epoch}, train loss = {train_loss}±{train_loss_std}, valid loss = {valid_loss}±{valid_loss_std}')\n",
    "\n",
    "    #if valid_loss==min(valid_losses):\n",
    "    if 0:\n",
    "        best_epoch = epoch\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'train_losses': train_losses,\n",
    "            'valid_losses': valid_losses,\n",
    "            }, ckpt_path)\n",
    "\n",
    "        logger.info(f'Current best epoch: {best_epoch}. Checkpoint saved.')\n",
    "\n",
    "    if epoch%epoches_save_loss == 0 and epoch!=0:\n",
    "        save_loss_data(train_losses, valid_losses, ckpt_dir)\n",
    "\n",
    "    if epoch-best_epoch>=epoches_adjust_lr and epoch-lr_updated_epoch>=epoches_adjust_lr_again:\n",
    "        adjust_lr(optimizer, gamma)\n",
    "        logger.info(f'Validation loss has not dropped for {epoch-best_epoch} epoches. Learning rate is decreased by a factor of {gamma}.')\n",
    "        lr_updated_epoch = epoch\n",
    "\n",
    "    #dataset_train.shuffle_indexinfile()\n",
    "    #dataset_train.shuffle_wflist()\n",
    "    train_loader = DataLoader(dataset_train, batch_size=batch_size_train // minibatch_size_train, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "rolled-voice",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "recal valid loss  tensor(-69.6999, device='cuda:1')\n"
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "with torch.no_grad():\n",
    "    for theta, x in valid_loader:\n",
    "\n",
    "        theta = theta[0:2].to(device)\n",
    "        x = x[0:2].to(device)\n",
    "        loss = -model.log_prob(theta, x).mean()\n",
    "        print('recal valid loss ', loss)\n",
    "\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "utility-cassette",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "recal valid loss  tensor(7133270., device='cuda:1')\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model.eval()\n",
    "\n",
    "    \n",
    "with torch.no_grad():\n",
    "    for theta, x in valid_loader:\n",
    "\n",
    "        theta = theta[0:2].to(device)\n",
    "        x = x[0:2].to(device)\n",
    "        loss = -model.log_prob(theta, x).mean()\n",
    "        print('recal valid loss ', loss)\n",
    "\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tamil-latex",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "conventional-tract",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 17])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "theta.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "quality-number",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3298545631232.0, 16470815.0, 2748165.0, 3105646.0, 3529984.5]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "lightweight-happening",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mysample_GlasNSFWarpper(model, dataset, detector_names=None, ipca_gen=None, device='cpu', Nsample=5000, max_event=1e3,\n",
    "                           batch_size=1):\n",
    "    model.eval()\n",
    "    loss_list = []\n",
    "    sample_list = []\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
    "    with torch.no_grad():\n",
    "        i = 0\n",
    "        for theta, x in dataloader:\n",
    "            theta = theta.to(device)\n",
    "            x = x.to(device)\n",
    "            \n",
    "            if type(dataset) == DatasetSVDStrainFDFromSVDWFonGPUBatch:\n",
    "                theta = theta.view(dataset.minibatch_size*batch_size, theta.shape[-1])\n",
    "                x = x.view(dataset.minibatch_size*batch_size, x.shape[-2], x.shape[-1])\n",
    "            \n",
    "            #print(theta)\n",
    "            lenx = x.shape[-1]\n",
    "            lentheta = theta.shape[-1]\n",
    "            loss = -model.log_prob(theta, x=x).mean()\n",
    "            #samples = model.sample(Nsample, x=x)\n",
    "\n",
    "\n",
    "            loss_list.append(loss.detach().cpu())\n",
    "            #sample_list.append(samples.cpu().numpy())\n",
    "            i+=1\n",
    "            if i>=max_event:\n",
    "                break\n",
    "    #samples = np.array(sample_list)\n",
    "    #samples = torch.from_numpy(samples)\n",
    "\n",
    "    #return samples.movedim(1,2), loss_list\n",
    "    return loss_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "experienced-honolulu",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "dataset_test = DatasetSVDStrainFDFromSVDWFonGPUBatch(train_filenames[0:1], PARAMETER_NAMES_CONTEXT_PRECESSINGBNS_BILBY, data_generator,\n",
    "                                     Nbasis=512, Vhfile=Vhfile, fix_extrinsic=True, shuffle=False, add_noise=False)\n",
    "#sample_list, loss_list =  mysample_GlasNSFWarpper(model, dataset_test, device=device, Nsample=5000, max_event=10e3, batch_size = 16384)\n",
    "loss_list =  mysample_GlasNSFWarpper(model, dataset_test, device=device, Nsample=5000,\n",
    "                                     max_event=10, batch_size = 1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "offshore-market",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 1.3860e+00,  5.0744e-01,  1.1953e-03,  4.1631e-02,  2.1004e+00,\n",
       "           1.1277e+00,  5.6505e+00,  6.1753e+00,  1.9041e+03, -5.9970e+02,\n",
       "           9.6461e-01,  1.0000e+02,  1.0000e+00,  1.0000e+00,  1.0000e+00,\n",
       "           3.0525e+00,  0.0000e+00]], device='cuda:0'),\n",
       " tensor([[[ 0.0371,  0.0506, -0.1011,  ..., -0.1928, -0.1159, -0.0336],\n",
       "          [ 0.0051, -0.0815, -0.0314,  ..., -0.3367,  0.0696, -0.1080],\n",
       "          [-0.0649,  0.0327,  0.1192,  ..., -0.1099,  0.0522, -0.0448],\n",
       "          [ 0.0399, -0.0800, -0.0875,  ...,  0.3880, -0.1349,  0.0475],\n",
       "          [-0.0760,  0.0218,  0.1387,  ...,  0.1063, -0.1521, -0.0273],\n",
       "          [-0.0412,  0.1007,  0.0600,  ...,  0.0143, -0.0500,  0.0155]]],\n",
       "        device='cuda:0'))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_test[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "soviet-drunk",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[1.3860e+00, 5.0744e-01, 1.1953e-03,  ..., 1.0000e+00, 3.0525e+00,\n",
       "          0.0000e+00],\n",
       "         [1.2603e+00, 7.4590e-01, 4.4495e-02,  ..., 1.0000e+00, 6.2830e+00,\n",
       "          0.0000e+00],\n",
       "         [2.0932e+00, 8.6074e-01, 6.6544e-02,  ..., 1.0000e+00, 1.0084e+00,\n",
       "          0.0000e+00],\n",
       "         ...,\n",
       "         [2.5070e+00, 9.6915e-01, 9.2409e-02,  ..., 1.0000e+00, 6.0114e+00,\n",
       "          0.0000e+00],\n",
       "         [1.3420e+00, 5.9567e-01, 6.0994e-02,  ..., 1.0000e+00, 3.4336e+00,\n",
       "          0.0000e+00],\n",
       "         [1.4278e+00, 5.1544e-01, 7.7941e-02,  ..., 1.0000e+00, 6.1844e-01,\n",
       "          0.0000e+00]], device='cuda:1'),\n",
       " tensor([[[ 3.7062e-02,  5.0639e-02, -1.0107e-01,  ..., -1.9278e-01,\n",
       "           -1.1593e-01, -3.3627e-02],\n",
       "          [ 5.0555e-03, -8.1532e-02, -3.1366e-02,  ..., -3.3670e-01,\n",
       "            6.9623e-02, -1.0800e-01],\n",
       "          [-6.4943e-02,  3.2658e-02,  1.1915e-01,  ..., -1.0990e-01,\n",
       "            5.2219e-02, -4.4834e-02],\n",
       "          [ 3.9880e-02, -7.9975e-02, -8.7511e-02,  ...,  3.8800e-01,\n",
       "           -1.3489e-01,  4.7518e-02],\n",
       "          [-7.5963e-02,  2.1835e-02,  1.3870e-01,  ...,  1.0625e-01,\n",
       "           -1.5210e-01, -2.7308e-02],\n",
       "          [-4.1174e-02,  1.0073e-01,  5.9998e-02,  ...,  1.4251e-02,\n",
       "           -5.0048e-02,  1.5513e-02]],\n",
       " \n",
       "         [[ 1.0483e-02, -5.6287e-02, -1.4446e-01,  ...,  2.7532e-01,\n",
       "            4.4453e-02,  6.5487e-02],\n",
       "          [ 4.1380e-03,  2.5605e-02,  1.9497e-01,  ...,  2.4251e-01,\n",
       "           -1.6142e-02,  9.1282e-02],\n",
       "          [ 8.5750e-02, -4.2790e-02,  1.5180e-02,  ..., -1.0966e-01,\n",
       "            4.1142e-02, -3.4061e-02],\n",
       "          [ 3.6148e-03, -3.2409e-02,  1.4611e-01,  ..., -2.9847e-01,\n",
       "            1.5399e-01, -5.1315e-02],\n",
       "          [-7.5251e-02,  4.2023e-02, -1.2697e-01,  ..., -1.8613e-01,\n",
       "            1.3513e-01, -6.2936e-02],\n",
       "          [ 7.7004e-02,  1.8758e-02,  2.4390e-01,  ...,  6.1401e-03,\n",
       "           -8.1210e-02,  2.5397e-02]],\n",
       " \n",
       "         [[ 7.3496e-01,  1.0489e+00,  1.0135e+00,  ...,  1.5023e-01,\n",
       "            7.7729e-02, -6.4032e-04],\n",
       "          [-4.1082e+00,  6.4075e-01,  1.6130e+00,  ...,  2.6244e-01,\n",
       "           -1.1456e-01, -9.7389e-03],\n",
       "          [ 8.2965e-01,  1.0758e+00, -1.9710e+00,  ...,  9.9130e-02,\n",
       "           -3.8245e-02, -1.3256e-03],\n",
       "          [-5.2025e+00, -9.0833e-02,  3.2757e+00,  ..., -2.1494e-01,\n",
       "            1.2356e-01,  4.2124e-03],\n",
       "          [ 1.7377e+00,  2.6919e-01, -2.3874e+00,  ..., -3.1662e-02,\n",
       "            8.6430e-02,  4.4782e-02],\n",
       "          [ 3.6364e+00, -7.0278e-01, -1.6922e+00,  ...,  1.0409e-02,\n",
       "            4.6197e-02,  1.5761e-02]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[ 3.2700e-01, -4.5968e+00,  1.4521e-01,  ...,  6.0441e-02,\n",
       "           -6.4208e-02, -6.6867e-02],\n",
       "          [ 2.0116e+00,  2.2841e+00,  3.1602e+00,  ..., -2.4888e-01,\n",
       "           -7.3323e-02, -2.2516e-02],\n",
       "          [ 3.9944e-01,  4.4108e+00, -2.8296e+00,  ..., -9.6536e-02,\n",
       "            3.0709e-02, -1.0725e-02],\n",
       "          [ 1.2453e+00,  1.2044e+00,  4.4665e+00,  ...,  2.8390e-01,\n",
       "           -1.1116e-01, -8.7038e-03],\n",
       "          [-5.0956e-01,  4.4162e+00, -3.2410e+00,  ...,  2.3008e-01,\n",
       "           -1.4309e-01, -4.7958e-03],\n",
       "          [-2.2149e+00, -4.1797e-01, -3.4160e+00,  ...,  8.8131e-02,\n",
       "           -3.1766e-02, -2.2405e-02]],\n",
       " \n",
       "         [[-1.0570e-02, -3.1967e-02, -7.7824e-02,  ..., -1.7934e-01,\n",
       "            4.4371e-02, -2.4934e-02],\n",
       "          [ 3.1792e-02, -5.4727e-03,  1.0464e-02,  ..., -3.6068e-02,\n",
       "            6.5053e-02,  7.4416e-03],\n",
       "          [ 2.7402e-02, -7.4259e-03,  8.2977e-02,  ...,  1.0628e-02,\n",
       "            1.3830e-02,  5.9165e-03],\n",
       "          [ 5.4434e-02, -8.2709e-04, -7.3813e-04,  ..., -1.6548e-02,\n",
       "           -5.5212e-02, -8.7517e-04],\n",
       "          [ 9.1145e-03,  8.0877e-03,  5.5679e-02,  ..., -1.4906e-01,\n",
       "            1.6916e-02, -3.1778e-02],\n",
       "          [-2.9708e-02,  2.2222e-02,  1.1326e-02,  ..., -7.1216e-02,\n",
       "            1.6382e-02, -2.1702e-02]],\n",
       " \n",
       "         [[ 1.0775e-02,  1.6472e-02, -4.3725e-02,  ...,  8.9980e-02,\n",
       "            1.8901e-02,  1.8666e-02],\n",
       "          [ 5.9066e-02, -1.0861e-02, -1.9852e-02,  ...,  1.0114e-01,\n",
       "            2.1074e-02,  1.8893e-02],\n",
       "          [-9.5029e-02, -1.1102e-01,  6.8026e-02,  ..., -5.9254e-02,\n",
       "            5.4222e-03, -1.6987e-02],\n",
       "          [-2.4133e-02, -8.0308e-02, -3.4359e-02,  ..., -1.3541e-01,\n",
       "            7.2638e-02,  4.8699e-03],\n",
       "          [ 2.0315e-02,  1.0134e-01, -2.1479e-02,  ..., -1.4202e-01,\n",
       "            7.8767e-02,  2.2897e-02],\n",
       "          [ 4.7806e-02, -3.8949e-02, -2.0497e-02,  ...,  3.6305e-02,\n",
       "           -4.1040e-02, -1.2185e-02]]], device='cuda:1'))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ultimate-stylus",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor(3530054.7500),\n",
       " tensor(3529944.2500),\n",
       " tensor(3530091.7500),\n",
       " tensor(3530026.7500),\n",
       " tensor(3530048.5000),\n",
       " tensor(3530029.7500),\n",
       " tensor(3529994.7500),\n",
       " tensor(3530016.5000),\n",
       " tensor(3529949.),\n",
       " tensor(3529895.2500)]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "welcome-bacon",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myigwn-py39",
   "language": "python",
   "name": "myigwn-py39"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
