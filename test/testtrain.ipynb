{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "considered-teaching",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<frozen importlib._bootstrap>:228: RuntimeWarning: scipy._lib.messagestream.MessageStream size changed, may indicate binary incompatibility. Expected 56 from C header, got 64 from PyObject\n",
      "/home/qian.hu/.local/lib/python3.9/site-packages/glue/ligolw/lsctables.py:50: UserWarning: Wswiglal-redir-stdio:\n",
      "\n",
      "SWIGLAL standard output/error redirection is enabled in IPython.\n",
      "This may lead to performance penalties. To disable locally, use:\n",
      "\n",
      "with lal.no_swig_redirect_standard_output_error():\n",
      "    ...\n",
      "\n",
      "To disable globally, use:\n",
      "\n",
      "lal.swig_redirect_standard_output_error(True)\n",
      "\n",
      "Note however that this will likely lead to error messages from\n",
      "LAL functions being either misdirected or lost when called from\n",
      "Jupyter notebooks.\n",
      "\n",
      "To suppress this warning, use:\n",
      "\n",
      "import warnings\n",
      "warnings.filterwarnings(\"ignore\", \"Wswiglal-redir-stdio\")\n",
      "import lal\n",
      "\n",
      "  import lal\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import bilby \n",
    "#import pycbc \n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "import glob \n",
    "\n",
    "#import zuko\n",
    "from glasflow import RealNVP, CouplingNSF\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch import nn\n",
    "\n",
    "import river.data\n",
    "from river.data.datagenerator import DataGeneratorBilbyFD\n",
    "from river.data.dataset import DatasetSVDStrainFDFromSVDWFonGPU, DatasetSVDStrainFDFromSVDWFonGPUBatch\n",
    "#import river.data.utils as datautils\n",
    "from river.data.utils import *\n",
    "\n",
    "from river.models import embedding\n",
    "from river.models.utils import *\n",
    "from river.models.embedding.conv import EmbeddingConv1D, EmbeddingConv2D\n",
    "from river.models.embedding.mlp import EmbeddingMLP1D\n",
    "from river.models.inference.cnf import GlasNSFConv1DRes, GlasNSFConv1D, GlasNSFTest, GlasflowEmbdding\n",
    "\n",
    "import logging\n",
    "import sys\n",
    "import os\n",
    "import json\n",
    "from copy import deepcopy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "painted-container",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21:40 bilby INFO    : Waveform generator initiated with\n",
      "  frequency_domain_source_model: bilby.gw.source.lal_binary_neutron_star\n",
      "  time_domain_source_model: None\n",
      "  parameter_conversion: bilby.gw.conversion.convert_to_lal_binary_black_hole_parameters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using bilby_default PSDs to generate data.\n"
     ]
    }
   ],
   "source": [
    "config_path = 'test_train_output'\n",
    "with open(f\"{config_path}/config.json\", 'r') as f:\n",
    "    config = json.load(f)\n",
    "\n",
    "config_datagenerator = config['data_generator_parameters']\n",
    "config_training = config['training_parameters']\n",
    "config_model = config['model_parameters']\n",
    "config_precaldata = config['precaldata_parameters']\n",
    "\n",
    "\n",
    "\n",
    "# Set up logger\n",
    "PID = os.getpid()\n",
    "device='cuda:1'\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "formatter = logging.Formatter('%(asctime)s | %(levelname)s | %(message)s')\n",
    "\n",
    "stdout_handler = logging.StreamHandler(sys.stdout)\n",
    "stdout_handler.setLevel(logging.DEBUG)\n",
    "stdout_handler.setFormatter(formatter)\n",
    "\n",
    "ckpt_dir = config['ckpt_dir']\n",
    "if not os.path.exists(ckpt_dir):\n",
    "    os.mkdir(ckpt_dir)\n",
    "    logger.warning(f\"{ckpt_dir} does not exist. Made dir {ckpt_dir}.\")\n",
    "\n",
    "logfilename = f\"{ckpt_dir}/logs.log\"\n",
    "file_handler = logging.FileHandler(logfilename)\n",
    "file_handler.setLevel(logging.DEBUG)\n",
    "file_handler.setFormatter(formatter)\n",
    "logger.addHandler(file_handler)\n",
    "ckpt_path = f'{ckpt_dir}/checkpoint.pickle'\n",
    "\n",
    "logger.info(f'PID={PID}.')\n",
    "logger.info(f'Output path: {ckpt_dir}')\n",
    "\n",
    "detector_names = config_datagenerator['detector_names']\n",
    "\n",
    "\n",
    "\n",
    "logger.info(f'Loading precalculated data.')\n",
    "train_filenames = glob.glob(f\"{config_precaldata['train']['folder']}/batch*/*.h5\")[:2]\n",
    "valid_filenames = glob.glob(f\"{config_precaldata['valid']['folder']}/batch*/*.h5\")\n",
    "#valid_filenames = glob.glob(f\"{config_precaldata['train']['folder']}/batch*/*.h5\")[-1:]\n",
    "#logger.info(f'{len(train_precaldata_filelist)}, {len(valid_precaldata_filelist)}')\n",
    "\n",
    "data_generator = DataGeneratorBilbyFD(**config_datagenerator)\n",
    "\n",
    "Vhfile = config_model['Vhfile']\n",
    "Nbasis = config_model['Nbasis']\n",
    "batch_size_train = config_training['batch_size_train']\n",
    "minibatch_size_train = config_training['minibatch_size_train']\n",
    "batch_size_valid = config_training['batch_size_valid']\n",
    "\n",
    "\n",
    "dataset_train = DatasetSVDStrainFDFromSVDWFonGPUBatch(train_filenames, PARAMETER_NAMES_CONTEXT_PRECESSINGBNS_BILBY, data_generator,\n",
    "                                 Nbasis=Nbasis, Vhfile=Vhfile, device=device, minibatch_size=minibatch_size_train, \n",
    "                                                      fix_extrinsic=True, shuffle=False, add_noise=False)\n",
    "\n",
    "dataset_valid = DatasetSVDStrainFDFromSVDWFonGPU(valid_filenames, PARAMETER_NAMES_CONTEXT_PRECESSINGBNS_BILBY, data_generator,\n",
    "                                 Nbasis=Nbasis, Vhfile=Vhfile, device=device, fix_extrinsic=True, shuffle=False, add_noise=False)\n",
    "\n",
    "\n",
    "#minibatch_size_valid = 256\n",
    "#dataset_valid = DatasetSVDStrainFDFromSVDWFonGPUBatch(valid_filenames, PARAMETER_NAMES_CONTEXT_PRECESSINGBNS_BILBY, data_generator,\n",
    "#                                 Nbasis=Nbasis, Vhfile=Vhfile, device=device, minibatch_size=minibatch_size_valid, \n",
    "#                                                      fix_extrinsic=True, shuffle=False, add_noise=False)\n",
    "\n",
    "\n",
    "Nsample = len(dataset_train)*minibatch_size_train\n",
    "Nvalid = len(dataset_valid)\n",
    "logger.info(f'Nsample: {Nsample}, Nvalid: {Nvalid}.')\n",
    "logger.info(f'batch_size_train: {batch_size_train}, batch_size_valid: {batch_size_valid}')\n",
    "\n",
    "batch_size_train = 16384\n",
    "#batch_size_valid = 1024\n",
    "train_loader = DataLoader(dataset_train, batch_size=batch_size_train // minibatch_size_train, shuffle=False)\n",
    "valid_loader = DataLoader(dataset_valid, batch_size=batch_size_valid, shuffle=False)\n",
    "#valid_loader = DataLoader(dataset_valid, batch_size=batch_size_valid // minibatch_size_valid, shuffle=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "graduate-merchandise",
   "metadata": {},
   "outputs": [],
   "source": [
    "config_dict = config.copy()\n",
    "config_dict['model_parameters']['embedding']['model'] = 'EmbeddingResConv1DMLP'\n",
    "NCOND = 64\n",
    "config_dict['model_parameters']['embedding']['nout'] = NCOND\n",
    "#config_dict['model_parameters']['embedding']['ndet'] = 3\n",
    "config_dict['model_parameters']['embedding']['nbasis'] = config_dict['model_parameters']['Nbasis']\n",
    "config_dict['model_parameters']['embedding']['conv_params'] = {\n",
    "        'in_channel':  [6,  3, ],\n",
    "        'out_channel': [3, 1],\n",
    "        'kernel_size': [16, 16, 16, 8, 8, 8, 8, 8, 8, 8, 4, 4, 4, 4, 2, 2, 2, 1],\n",
    "        'stride':      [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
    "        'padding':     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "        'dilation':    [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
    "        'dropout':     [0.1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "    }\n",
    "config_dict['model_parameters']['embedding']['mlp_params'] = {\n",
    "        'in_features': [0,],\n",
    "        'out_features': [NCOND,],\n",
    "    }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "worldwide-posting",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "associate-memorial",
   "metadata": {},
   "outputs": [],
   "source": [
    "config_dict['model_parameters']['flow'] = {}\n",
    "\n",
    "config_dict['model_parameters']['flow']['model'] = 'CouplingNSF'\n",
    "\n",
    "#fixed: ra dec psi tc \n",
    "config_dict['model_parameters']['flow']['n_inputs'] = 12\n",
    "config_dict['model_parameters']['flow']['n_transforms'] = 3\n",
    "config_dict['model_parameters']['flow']['n_conditional_inputs'] = NCOND\n",
    "config_dict['model_parameters']['flow']['n_neurons'] = 6  # 32 by default\n",
    "config_dict['model_parameters']['flow']['batch_norm_between_transforms'] = True\n",
    "config_dict['model_parameters']['flow']['batch_norm_within_blocks'] = True\n",
    "config_dict['model_parameters']['flow']['n_blocks_per_transform'] = 2  # 2 by default, 5\n",
    "config_dict['model_parameters']['flow']['num_bins'] = 4  # 4 by default, 8\n",
    "config_dict['model_parameters']['flow']['tail_bound'] = 1 # 5 by default, 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "suffering-chain",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized MLP in channel: 482\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#model = GlasNSFConv1DRes(config).to(device)\n",
    "#model = GlasNSFConv1D(config).to(device)\n",
    "model = GlasflowEmbdding(config_dict).to(device)\n",
    "\n",
    "\n",
    "lr = config_training['lr']\n",
    "gamma = config_training['gamma']\n",
    "weight_decay = config_training['weight_decay']\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "logger.info(f'Initial learning rate: {lr}')\n",
    "logger.info(f'Gamma: {gamma}')\n",
    "\n",
    "max_epoch = config_training['max_epoch']\n",
    "#epoches_pretrain = config_training['epoches_pretrain']\n",
    "epoches_save_loss = config_training['epoches_save_loss']\n",
    "epoches_adjust_lr = config_training['epoches_adjust_lr']\n",
    "epoches_adjust_lr_again = config_training['epoches_adjust_lr_again']\n",
    "#load_from_previous_train = 1\n",
    "\n",
    "load_from_previous_train = False\n",
    "if load_from_previous_train:\n",
    "    checkpoint = torch.load(ckpt_path)\n",
    "\n",
    "    best_epoch = checkpoint['epoch']\n",
    "    start_epoch = best_epoch + 1\n",
    "    lr_updated_epoch = start_epoch\n",
    "    model.load_state_dict(checkpoint['model_state_dict']) \n",
    "\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "\n",
    "\n",
    "    train_losses = checkpoint['train_losses']\n",
    "    valid_losses = checkpoint['valid_losses']\n",
    "\n",
    "\n",
    "    logger.info(f'Loaded states from {ckpt_path}, epoch={start_epoch}.')\n",
    "else:\n",
    "    best_epoch = 0\n",
    "    train_losses = []\n",
    "    valid_losses = []\n",
    "\n",
    "    start_epoch = 0\n",
    "    lr_updated_epoch = start_epoch\n",
    "\n",
    "npara_flow = count_parameters(model.flow)\n",
    "#npara_embd_proj = count_parameters(model.embedding)\n",
    "#npara_embd_res = count_parameters(model.resnet)\n",
    "npara_total = count_parameters(model)\n",
    "#logger.info(f'Learnable parameters: flow: {npara_flow}, embedding_PCA: {npara_embd_proj}, ResNet: {npara_embd_res}. Total: {npara_total}. ')\n",
    "#logger.info(f'Learnable parameters: flow: {npara_flow}, embedding_PCA: {npara_embd_proj}. Total: {npara_total}. ')\n",
    "logger.info(f'Learnable parameters: flow: {npara_flow}, total: {npara_total}. ')\n",
    "\n",
    "###\n",
    "#for g in optimizer.param_groups:\n",
    "#    g['lr'] = 1e-5\n",
    "#    logger.info(f'Set lr to 1e-5.')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "equivalent-responsibility",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "advised-indianapolis",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Learnable parameters: flow: 5580, total: 37202. '"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f'Learnable parameters: flow: {npara_flow}, total: {npara_total}. '"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "located-excuse",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start_epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fluid-snapshot",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mytrain_GlasNSFWarpper(model, optimizer, dataloader, detector_names=None, ipca_gen=None, device='cpu',downsample_rate=1, minibatch_size=0):\n",
    "    model.train()\n",
    "    loss_list = []\n",
    "    for theta, x in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        theta = theta.to(device)\n",
    "        x = x.to(device)\n",
    "        \n",
    "        if minibatch_size>0:\n",
    "            # x: [bs, minibatch_size, nchannel, nbasis]\n",
    "            # theta: [bs, minibatch_size, npara]\n",
    "            bs = x.shape[0]\n",
    "            nbasis = x.shape[-1]\n",
    "            nchannel = x.shape[-2]\n",
    "            npara = theta.shape[-1]\n",
    "            theta = theta.view(bs*minibatch_size, npara)\n",
    "            x = x.view(bs*minibatch_size, nchannel, nbasis)\n",
    "        loss = -model.log_prob(theta, x).mean()\n",
    "        print('train loss ', loss)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        loss_list.append(loss.detach())\n",
    "\n",
    "    mean_loss = torch.stack(loss_list).mean().item() # mean(list of mean losses of each batch)\n",
    "    std_loss = torch.stack(loss_list).std().item()\n",
    "    return mean_loss, std_loss\n",
    "\n",
    "def myeval_GlasNSFWarpper(model, dataloader, detector_names=None, ipca_gen=None, device='cpu',downsample_rate=1):\n",
    "    model.eval()\n",
    "    loss_list = []\n",
    "    with torch.no_grad():\n",
    "        i=0\n",
    "        for theta, x in dataloader:\n",
    "            theta = theta.to(device)\n",
    "            x = x.to(device)\n",
    "            loss = -model.log_prob(theta, x).mean()\n",
    "            print('valid loss ', loss)\n",
    "            loss_list.append(loss.detach())\n",
    "            i+=1\n",
    "            if i>1:\n",
    "                break\n",
    "\n",
    "    mean_loss = torch.stack(loss_list).mean().item()\n",
    "    std_loss = torch.stack(loss_list).std().item()\n",
    "    return mean_loss, std_loss\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "relative-paintball",
   "metadata": {},
   "outputs": [],
   "source": [
    "adjust_lr(optimizer, 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "adjusted-expense",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "hazardous-gross",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss  tensor(26.4129, device='cuda:1', grad_fn=<NegBackward0>)\n",
      "train loss  tensor(26.3458, device='cuda:1', grad_fn=<NegBackward0>)\n",
      "train loss  tensor(26.1332, device='cuda:1', grad_fn=<NegBackward0>)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-19486ca80614>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m#for epoch in range(start_epoch, max_epoch):\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_epoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mtrain_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loss_std\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmytrain_GlasNSFWarpper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mminibatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mminibatch_size_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0mvalid_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_loss_std\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmyeval_GlasNSFWarpper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;31m#valid_loss, valid_loss_std = myeval_GlasNSFWarpper2(model, valid_loader, device=device, minibatch_size=minibatch_size_valid)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-14-3c45fabf02a8>\u001b[0m in \u001b[0;36mmytrain_GlasNSFWarpper\u001b[0;34m(model, optimizer, dataloader, detector_names, ipca_gen, device, downsample_rate, minibatch_size)\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mloss_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mtheta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdataloader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0mtheta\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtheta\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/myigwn-py39/lib/python3.9/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    626\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    627\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 628\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    629\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    630\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/myigwn-py39/lib/python3.9/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    669\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    670\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 671\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    672\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    673\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/myigwn-py39/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     56\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/myigwn-py39/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     56\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/mlpe/river/river/data/dataset.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    615\u001b[0m             \u001b[0mwf_dict_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_precalwf_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    616\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 617\u001b[0;31m         \u001b[0mhp_svd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhc_svd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_waveform_tensors_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwf_dict_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex_in_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex_in_file_end\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    618\u001b[0m         \u001b[0minjection_parameters\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_injection_parameters_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwf_dict_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mindex_in_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex_in_file_end\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m         \u001b[0minjection_parameters\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_injection_parameters_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minjection_parameters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/mlpe/river/river/data/dataset.py\u001b[0m in \u001b[0;36mget_waveform_tensors_batch\u001b[0;34m(self, wf_dict_list, index_in_file, index_in_file_end)\u001b[0m\n\u001b[1;32m    661\u001b[0m                     torch.exp(1j*torch.from_numpy(wf_dict['waveform_polarizations']['plus']['phase'][index])).type(torch.complex64)).to(self.device)\n\u001b[1;32m    662\u001b[0m                 hc_svd = (torch.from_numpy(wf_dict['waveform_polarizations']['cross']['amplitude'][index]) *\\\n\u001b[0;32m--> 663\u001b[0;31m                     torch.exp(1j*torch.from_numpy(wf_dict['waveform_polarizations']['cross']['phase'][index])).type(torch.complex64)).to(self.device)\n\u001b[0m\u001b[1;32m    664\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    665\u001b[0m                 \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom_index_in_file\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex_in_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mend_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "logger.info(f'Training started, device:{device}. ')\n",
    "\n",
    "max_epoch = 200\n",
    "#for epoch in range(start_epoch, max_epoch):    \n",
    "for epoch in range(8, max_epoch):   \n",
    "    train_loss, train_loss_std = mytrain_GlasNSFWarpper(model, optimizer, train_loader, device=device, minibatch_size=minibatch_size_train)\n",
    "    valid_loss, valid_loss_std = myeval_GlasNSFWarpper(model, valid_loader, device=device)\n",
    "    #valid_loss, valid_loss_std = myeval_GlasNSFWarpper2(model, valid_loader, device=device, minibatch_size=minibatch_size_valid)\n",
    "    \n",
    "    #\n",
    "    model.train()\n",
    "    with torch.no_grad():\n",
    "        for theta, x in train_loader:\n",
    "\n",
    "            theta = theta.to(device)\n",
    "            x = x.to(device)\n",
    "            bs = x.shape[0]\n",
    "            nbasis = x.shape[-1]\n",
    "            nchannel = x.shape[-2]\n",
    "            npara = theta.shape[-1]\n",
    "            theta = theta.view(bs*minibatch_size_train, npara)\n",
    "            x = x.view(bs*minibatch_size_train, nchannel, nbasis)\n",
    "\n",
    "            loss = -model.log_prob(theta, x).mean()\n",
    "            print('recal train loss ', loss)\n",
    "\n",
    "            break\n",
    "        \n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for theta, x in valid_loader:\n",
    "            \n",
    "            \n",
    "            \n",
    "            #bs = x.shape[0]\n",
    "            #nbasis = x.shape[-1]\n",
    "            #nchannel = x.shape[-2]\n",
    "            #npara = theta.shape[-1]\n",
    "            #theta = theta.view(bs*minibatch_size_valid, npara)\n",
    "            #x = x.view(bs*minibatch_size_valid, nchannel, nbasis)\n",
    "            \n",
    "            theta = theta[0:2].to(device)\n",
    "            x = x[0:2].to(device)\n",
    "            \n",
    "            loss = -model.log_prob(theta, x).mean()\n",
    "            print('recal valid loss ', loss)\n",
    "            \n",
    "            break\n",
    "            \n",
    "    train_losses.append(train_loss)\n",
    "    valid_losses.append(valid_loss)\n",
    "\n",
    "    logger.info(f'epoch {epoch}, train loss = {train_loss}±{train_loss_std}, valid loss = {valid_loss}±{valid_loss_std}')\n",
    "\n",
    "    #if valid_loss==min(valid_losses):\n",
    "    if 0:\n",
    "        best_epoch = epoch\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'train_losses': train_losses,\n",
    "            'valid_losses': valid_losses,\n",
    "            }, ckpt_path)\n",
    "\n",
    "        logger.info(f'Current best epoch: {best_epoch}. Checkpoint saved.')\n",
    "\n",
    "    if epoch%epoches_save_loss == 0 and epoch!=0:\n",
    "        save_loss_data(train_losses, valid_losses, ckpt_dir)\n",
    "\n",
    "    if epoch-best_epoch>=epoches_adjust_lr and epoch-lr_updated_epoch>=epoches_adjust_lr_again:\n",
    "        adjust_lr(optimizer, gamma)\n",
    "        logger.info(f'Validation loss has not dropped for {epoch-best_epoch} epoches. Learning rate is decreased by a factor of {gamma}.')\n",
    "        lr_updated_epoch = epoch\n",
    "\n",
    "    #dataset_train.shuffle_indexinfile()\n",
    "    #dataset_train.shuffle_wflist()\n",
    "    train_loader = DataLoader(dataset_train, batch_size=batch_size_train // minibatch_size_train, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "spoken-baker",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "rolled-voice",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "recal valid loss  tensor(-69.6999, device='cuda:1')\n"
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "with torch.no_grad():\n",
    "    for theta, x in valid_loader:\n",
    "\n",
    "        theta = theta[0:2].to(device)\n",
    "        x = x[0:2].to(device)\n",
    "        loss = -model.log_prob(theta, x).mean()\n",
    "        print('recal valid loss ', loss)\n",
    "\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "utility-cassette",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "recal valid loss  tensor(7133270., device='cuda:1')\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model.eval()\n",
    "\n",
    "    \n",
    "with torch.no_grad():\n",
    "    for theta, x in valid_loader:\n",
    "\n",
    "        theta = theta[0:2].to(device)\n",
    "        x = x[0:2].to(device)\n",
    "        loss = -model.log_prob(theta, x).mean()\n",
    "        print('recal valid loss ', loss)\n",
    "\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tamil-latex",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "conventional-tract",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 17])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "theta.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "quality-number",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3298545631232.0, 16470815.0, 2748165.0, 3105646.0, 3529984.5]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "lightweight-happening",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mysample_GlasNSFWarpper(model, dataset, detector_names=None, ipca_gen=None, device='cpu', Nsample=5000, max_event=1e3,\n",
    "                           batch_size=1):\n",
    "    model.eval()\n",
    "    loss_list = []\n",
    "    sample_list = []\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
    "    with torch.no_grad():\n",
    "        i = 0\n",
    "        for theta, x in dataloader:\n",
    "            theta = theta.to(device)\n",
    "            x = x.to(device)\n",
    "            \n",
    "            if type(dataset) == DatasetSVDStrainFDFromSVDWFonGPUBatch:\n",
    "                theta = theta.view(dataset.minibatch_size*batch_size, theta.shape[-1])\n",
    "                x = x.view(dataset.minibatch_size*batch_size, x.shape[-2], x.shape[-1])\n",
    "            \n",
    "            #print(theta)\n",
    "            lenx = x.shape[-1]\n",
    "            lentheta = theta.shape[-1]\n",
    "            loss = -model.log_prob(theta, x=x).mean()\n",
    "            #samples = model.sample(Nsample, x=x)\n",
    "\n",
    "\n",
    "            loss_list.append(loss.detach().cpu())\n",
    "            #sample_list.append(samples.cpu().numpy())\n",
    "            i+=1\n",
    "            if i>=max_event:\n",
    "                break\n",
    "    #samples = np.array(sample_list)\n",
    "    #samples = torch.from_numpy(samples)\n",
    "\n",
    "    #return samples.movedim(1,2), loss_list\n",
    "    return loss_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "experienced-honolulu",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "dataset_test = DatasetSVDStrainFDFromSVDWFonGPUBatch(train_filenames[0:1], PARAMETER_NAMES_CONTEXT_PRECESSINGBNS_BILBY, data_generator,\n",
    "                                     Nbasis=512, Vhfile=Vhfile, fix_extrinsic=True, shuffle=False, add_noise=False)\n",
    "#sample_list, loss_list =  mysample_GlasNSFWarpper(model, dataset_test, device=device, Nsample=5000, max_event=10e3, batch_size = 16384)\n",
    "loss_list =  mysample_GlasNSFWarpper(model, dataset_test, device=device, Nsample=5000,\n",
    "                                     max_event=10, batch_size = 1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "offshore-market",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 1.3860e+00,  5.0744e-01,  1.1953e-03,  4.1631e-02,  2.1004e+00,\n",
       "           1.1277e+00,  5.6505e+00,  6.1753e+00,  1.9041e+03, -5.9970e+02,\n",
       "           9.6461e-01,  1.0000e+02,  1.0000e+00,  1.0000e+00,  1.0000e+00,\n",
       "           3.0525e+00,  0.0000e+00]], device='cuda:0'),\n",
       " tensor([[[ 0.0371,  0.0506, -0.1011,  ..., -0.1928, -0.1159, -0.0336],\n",
       "          [ 0.0051, -0.0815, -0.0314,  ..., -0.3367,  0.0696, -0.1080],\n",
       "          [-0.0649,  0.0327,  0.1192,  ..., -0.1099,  0.0522, -0.0448],\n",
       "          [ 0.0399, -0.0800, -0.0875,  ...,  0.3880, -0.1349,  0.0475],\n",
       "          [-0.0760,  0.0218,  0.1387,  ...,  0.1063, -0.1521, -0.0273],\n",
       "          [-0.0412,  0.1007,  0.0600,  ...,  0.0143, -0.0500,  0.0155]]],\n",
       "        device='cuda:0'))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_test[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "soviet-drunk",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[1.3860e+00, 5.0744e-01, 1.1953e-03,  ..., 1.0000e+00, 3.0525e+00,\n",
       "          0.0000e+00],\n",
       "         [1.2603e+00, 7.4590e-01, 4.4495e-02,  ..., 1.0000e+00, 6.2830e+00,\n",
       "          0.0000e+00],\n",
       "         [2.0932e+00, 8.6074e-01, 6.6544e-02,  ..., 1.0000e+00, 1.0084e+00,\n",
       "          0.0000e+00],\n",
       "         ...,\n",
       "         [2.5070e+00, 9.6915e-01, 9.2409e-02,  ..., 1.0000e+00, 6.0114e+00,\n",
       "          0.0000e+00],\n",
       "         [1.3420e+00, 5.9567e-01, 6.0994e-02,  ..., 1.0000e+00, 3.4336e+00,\n",
       "          0.0000e+00],\n",
       "         [1.4278e+00, 5.1544e-01, 7.7941e-02,  ..., 1.0000e+00, 6.1844e-01,\n",
       "          0.0000e+00]], device='cuda:1'),\n",
       " tensor([[[ 3.7062e-02,  5.0639e-02, -1.0107e-01,  ..., -1.9278e-01,\n",
       "           -1.1593e-01, -3.3627e-02],\n",
       "          [ 5.0555e-03, -8.1532e-02, -3.1366e-02,  ..., -3.3670e-01,\n",
       "            6.9623e-02, -1.0800e-01],\n",
       "          [-6.4943e-02,  3.2658e-02,  1.1915e-01,  ..., -1.0990e-01,\n",
       "            5.2219e-02, -4.4834e-02],\n",
       "          [ 3.9880e-02, -7.9975e-02, -8.7511e-02,  ...,  3.8800e-01,\n",
       "           -1.3489e-01,  4.7518e-02],\n",
       "          [-7.5963e-02,  2.1835e-02,  1.3870e-01,  ...,  1.0625e-01,\n",
       "           -1.5210e-01, -2.7308e-02],\n",
       "          [-4.1174e-02,  1.0073e-01,  5.9998e-02,  ...,  1.4251e-02,\n",
       "           -5.0048e-02,  1.5513e-02]],\n",
       " \n",
       "         [[ 1.0483e-02, -5.6287e-02, -1.4446e-01,  ...,  2.7532e-01,\n",
       "            4.4453e-02,  6.5487e-02],\n",
       "          [ 4.1380e-03,  2.5605e-02,  1.9497e-01,  ...,  2.4251e-01,\n",
       "           -1.6142e-02,  9.1282e-02],\n",
       "          [ 8.5750e-02, -4.2790e-02,  1.5180e-02,  ..., -1.0966e-01,\n",
       "            4.1142e-02, -3.4061e-02],\n",
       "          [ 3.6148e-03, -3.2409e-02,  1.4611e-01,  ..., -2.9847e-01,\n",
       "            1.5399e-01, -5.1315e-02],\n",
       "          [-7.5251e-02,  4.2023e-02, -1.2697e-01,  ..., -1.8613e-01,\n",
       "            1.3513e-01, -6.2936e-02],\n",
       "          [ 7.7004e-02,  1.8758e-02,  2.4390e-01,  ...,  6.1401e-03,\n",
       "           -8.1210e-02,  2.5397e-02]],\n",
       " \n",
       "         [[ 7.3496e-01,  1.0489e+00,  1.0135e+00,  ...,  1.5023e-01,\n",
       "            7.7729e-02, -6.4032e-04],\n",
       "          [-4.1082e+00,  6.4075e-01,  1.6130e+00,  ...,  2.6244e-01,\n",
       "           -1.1456e-01, -9.7389e-03],\n",
       "          [ 8.2965e-01,  1.0758e+00, -1.9710e+00,  ...,  9.9130e-02,\n",
       "           -3.8245e-02, -1.3256e-03],\n",
       "          [-5.2025e+00, -9.0833e-02,  3.2757e+00,  ..., -2.1494e-01,\n",
       "            1.2356e-01,  4.2124e-03],\n",
       "          [ 1.7377e+00,  2.6919e-01, -2.3874e+00,  ..., -3.1662e-02,\n",
       "            8.6430e-02,  4.4782e-02],\n",
       "          [ 3.6364e+00, -7.0278e-01, -1.6922e+00,  ...,  1.0409e-02,\n",
       "            4.6197e-02,  1.5761e-02]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[ 3.2700e-01, -4.5968e+00,  1.4521e-01,  ...,  6.0441e-02,\n",
       "           -6.4208e-02, -6.6867e-02],\n",
       "          [ 2.0116e+00,  2.2841e+00,  3.1602e+00,  ..., -2.4888e-01,\n",
       "           -7.3323e-02, -2.2516e-02],\n",
       "          [ 3.9944e-01,  4.4108e+00, -2.8296e+00,  ..., -9.6536e-02,\n",
       "            3.0709e-02, -1.0725e-02],\n",
       "          [ 1.2453e+00,  1.2044e+00,  4.4665e+00,  ...,  2.8390e-01,\n",
       "           -1.1116e-01, -8.7038e-03],\n",
       "          [-5.0956e-01,  4.4162e+00, -3.2410e+00,  ...,  2.3008e-01,\n",
       "           -1.4309e-01, -4.7958e-03],\n",
       "          [-2.2149e+00, -4.1797e-01, -3.4160e+00,  ...,  8.8131e-02,\n",
       "           -3.1766e-02, -2.2405e-02]],\n",
       " \n",
       "         [[-1.0570e-02, -3.1967e-02, -7.7824e-02,  ..., -1.7934e-01,\n",
       "            4.4371e-02, -2.4934e-02],\n",
       "          [ 3.1792e-02, -5.4727e-03,  1.0464e-02,  ..., -3.6068e-02,\n",
       "            6.5053e-02,  7.4416e-03],\n",
       "          [ 2.7402e-02, -7.4259e-03,  8.2977e-02,  ...,  1.0628e-02,\n",
       "            1.3830e-02,  5.9165e-03],\n",
       "          [ 5.4434e-02, -8.2709e-04, -7.3813e-04,  ..., -1.6548e-02,\n",
       "           -5.5212e-02, -8.7517e-04],\n",
       "          [ 9.1145e-03,  8.0877e-03,  5.5679e-02,  ..., -1.4906e-01,\n",
       "            1.6916e-02, -3.1778e-02],\n",
       "          [-2.9708e-02,  2.2222e-02,  1.1326e-02,  ..., -7.1216e-02,\n",
       "            1.6382e-02, -2.1702e-02]],\n",
       " \n",
       "         [[ 1.0775e-02,  1.6472e-02, -4.3725e-02,  ...,  8.9980e-02,\n",
       "            1.8901e-02,  1.8666e-02],\n",
       "          [ 5.9066e-02, -1.0861e-02, -1.9852e-02,  ...,  1.0114e-01,\n",
       "            2.1074e-02,  1.8893e-02],\n",
       "          [-9.5029e-02, -1.1102e-01,  6.8026e-02,  ..., -5.9254e-02,\n",
       "            5.4222e-03, -1.6987e-02],\n",
       "          [-2.4133e-02, -8.0308e-02, -3.4359e-02,  ..., -1.3541e-01,\n",
       "            7.2638e-02,  4.8699e-03],\n",
       "          [ 2.0315e-02,  1.0134e-01, -2.1479e-02,  ..., -1.4202e-01,\n",
       "            7.8767e-02,  2.2897e-02],\n",
       "          [ 4.7806e-02, -3.8949e-02, -2.0497e-02,  ...,  3.6305e-02,\n",
       "           -4.1040e-02, -1.2185e-02]]], device='cuda:1'))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ultimate-stylus",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor(3530054.7500),\n",
       " tensor(3529944.2500),\n",
       " tensor(3530091.7500),\n",
       " tensor(3530026.7500),\n",
       " tensor(3530048.5000),\n",
       " tensor(3530029.7500),\n",
       " tensor(3529994.7500),\n",
       " tensor(3530016.5000),\n",
       " tensor(3529949.),\n",
       " tensor(3529895.2500)]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "welcome-bacon",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myigwn-py39",
   "language": "python",
   "name": "myigwn-py39"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
