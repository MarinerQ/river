{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "blocked-thermal",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "from einops import rearrange\n",
    "from einops.layers.torch import Rearrange\n",
    "\n",
    "# helpers\n",
    "\n",
    "def posemb_sincos_1d(patches, temperature = 10000, dtype = torch.float32):\n",
    "    _, n, dim, device, dtype = *patches.shape, patches.device, patches.dtype\n",
    "\n",
    "    n = torch.arange(n, device = device)\n",
    "    assert (dim % 2) == 0, 'feature dimension must be multiple of 2 for sincos emb'\n",
    "    omega = torch.arange(dim // 2, device = device) / (dim // 2 - 1)\n",
    "    omega = 1. / (temperature ** omega)\n",
    "\n",
    "    n = n.flatten()[:, None] * omega[None, :]\n",
    "    pe = torch.cat((n.sin(), n.cos()), dim = 1)\n",
    "    return pe.type(dtype)\n",
    "\n",
    "# classes\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, dim, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.LayerNorm(dim),\n",
    "            nn.Linear(dim, hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(hidden_dim, dim),\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, dim, heads = 8, dim_head = 64):\n",
    "        super().__init__()\n",
    "        inner_dim = dim_head *  heads\n",
    "        self.heads = heads\n",
    "        self.scale = dim_head ** -0.5\n",
    "        self.norm = nn.LayerNorm(dim)\n",
    "\n",
    "        self.attend = nn.Softmax(dim = -1)\n",
    "\n",
    "        self.to_qkv = nn.Linear(dim, inner_dim * 3, bias = False)\n",
    "        self.to_out = nn.Linear(inner_dim, dim, bias = False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.norm(x)\n",
    "\n",
    "        qkv = self.to_qkv(x).chunk(3, dim = -1)\n",
    "        q, k, v = map(lambda t: rearrange(t, 'b n (h d) -> b h n d', h = self.heads), qkv)\n",
    "\n",
    "        dots = torch.matmul(q, k.transpose(-1, -2)) * self.scale\n",
    "\n",
    "        attn = self.attend(dots)\n",
    "\n",
    "        out = torch.matmul(attn, v)\n",
    "        out = rearrange(out, 'b h n d -> b n (h d)')\n",
    "        return self.to_out(out)\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, dim, depth, heads, dim_head, mlp_dim):\n",
    "        super().__init__()\n",
    "        self.norm = nn.LayerNorm(dim)\n",
    "        self.layers = nn.ModuleList([])\n",
    "        for _ in range(depth):\n",
    "            self.layers.append(nn.ModuleList([\n",
    "                Attention(dim, heads = heads, dim_head = dim_head),\n",
    "                FeedForward(dim, mlp_dim)\n",
    "            ]))\n",
    "    def forward(self, x):\n",
    "        for attn, ff in self.layers:\n",
    "            x = attn(x) + x\n",
    "            x = ff(x) + x\n",
    "        return self.norm(x)\n",
    "\n",
    "class SimpleViT(nn.Module):\n",
    "    def __init__(self, *, seq_len, patch_size, num_classes, dim, depth, heads, mlp_dim, channels = 3, dim_head = 64):\n",
    "        super().__init__()\n",
    "\n",
    "        assert seq_len % patch_size == 0\n",
    "\n",
    "        num_patches = seq_len // patch_size\n",
    "        patch_dim = channels * patch_size\n",
    "\n",
    "        self.to_patch_embedding = nn.Sequential(\n",
    "            Rearrange('b c (n p) -> b n (p c)', p = patch_size),\n",
    "            nn.LayerNorm(patch_dim),\n",
    "            nn.Linear(patch_dim, dim),\n",
    "            nn.LayerNorm(dim),\n",
    "        )\n",
    "\n",
    "        self.transformer = Transformer(dim, depth, heads, dim_head, mlp_dim)\n",
    "\n",
    "        self.to_latent = nn.Identity()\n",
    "        self.linear_head = nn.Linear(dim, num_classes)\n",
    "\n",
    "    def forward(self, series):\n",
    "        *_, n, dtype = *series.shape, series.dtype\n",
    "\n",
    "        x = self.to_patch_embedding(series)\n",
    "        pe = posemb_sincos_1d(x)\n",
    "        x = rearrange(x, 'b ... d -> b (...) d') + pe\n",
    "\n",
    "        x = self.transformer(x)\n",
    "        x = x.mean(dim = 1)\n",
    "\n",
    "        x = self.to_latent(x)\n",
    "        return self.linear_head(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "domestic-fence",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "sealed-remove",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10240"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "256*40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dirty-chess",
   "metadata": {},
   "outputs": [],
   "source": [
    "20 -> 1024\n",
    "each band 256 points\n",
    "-> 13 bands # safefactor=2?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "acting-vitamin",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3328"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "256*13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "unusual-setup",
   "metadata": {},
   "outputs": [],
   "source": [
    "v = SimpleViT(\n",
    "        seq_len = 3328,\n",
    "        patch_size = 256,\n",
    "        num_classes = 256,\n",
    "        dim = 1024,\n",
    "        depth = 6,\n",
    "        heads = 8,\n",
    "        mlp_dim = 2048,\n",
    "        channels = 6, \n",
    "        dim_head = 64,\n",
    "    ).to('cuda')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dramatic-vegetation",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "accredited-clinton",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_series = torch.randn(1024, 6, 3328).to('cuda')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "false-lafayette",
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = v(time_series)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "broad-lender",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1024, 256])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "successful-norman",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Mar 28 03:39:55 2024       \r\n",
      "+-----------------------------------------------------------------------------------------+\r\n",
      "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\r\n",
      "|-----------------------------------------+------------------------+----------------------+\r\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\r\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\r\n",
      "|                                         |                        |               MIG M. |\r\n",
      "|=========================================+========================+======================|\r\n",
      "|   0  NVIDIA GeForce RTX 3080        Off |   00000000:01:00.0 Off |                  N/A |\r\n",
      "|  0%   15C    P8             14W /  320W |       3MiB /  10240MiB |      0%      Default |\r\n",
      "|                                         |                        |                  N/A |\r\n",
      "+-----------------------------------------+------------------------+----------------------+\r\n",
      "|   1  NVIDIA A100-PCIE-40GB          Off |   00000000:C1:00.0 Off |                    0 |\r\n",
      "| N/A   24C    P0             32W /  250W |   17602MiB /  40960MiB |      0%      Default |\r\n",
      "|                                         |                        |             Disabled |\r\n",
      "+-----------------------------------------+------------------------+----------------------+\r\n",
      "                                                                                         \r\n",
      "+-----------------------------------------------------------------------------------------+\r\n",
      "| Processes:                                                                              |\r\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\r\n",
      "|        ID   ID                                                               Usage      |\r\n",
      "|=========================================================================================|\r\n",
      "|    1   N/A  N/A   1170504      C   .../.conda/envs/myigwn-py39/bin/python       4450MiB |\r\n",
      "|    1   N/A  N/A   1645941      C   .../.conda/envs/myigwn-py39/bin/python       1072MiB |\r\n",
      "|    1   N/A  N/A   2451206      C   python                                      12058MiB |\r\n",
      "+-----------------------------------------------------------------------------------------+\r\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "devoted-leisure",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "miniature-belly",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "39635200"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_parameters(v)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "generous-silver",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000000"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1000 * 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "intimate-genre",
   "metadata": {},
   "source": [
    "# train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "crazy-throat",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<frozen importlib._bootstrap>:228: RuntimeWarning: scipy._lib.messagestream.MessageStream size changed, may indicate binary incompatibility. Expected 56 from C header, got 64 from PyObject\n",
      "/home/qian.hu/.local/lib/python3.9/site-packages/glue/ligolw/lsctables.py:50: UserWarning: Wswiglal-redir-stdio:\n",
      "\n",
      "SWIGLAL standard output/error redirection is enabled in IPython.\n",
      "This may lead to performance penalties. To disable locally, use:\n",
      "\n",
      "with lal.no_swig_redirect_standard_output_error():\n",
      "    ...\n",
      "\n",
      "To disable globally, use:\n",
      "\n",
      "lal.swig_redirect_standard_output_error(True)\n",
      "\n",
      "Note however that this will likely lead to error messages from\n",
      "LAL functions being either misdirected or lost when called from\n",
      "Jupyter notebooks.\n",
      "\n",
      "To suppress this warning, use:\n",
      "\n",
      "import warnings\n",
      "warnings.filterwarnings(\"ignore\", \"Wswiglal-redir-stdio\")\n",
      "import lal\n",
      "\n",
      "  import lal\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import numpy as np\n",
    "import bilby \n",
    "#import pycbc \n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "import glob \n",
    "\n",
    "#import zuko\n",
    "from glasflow import RealNVP, CouplingNSF\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch import nn\n",
    "\n",
    "import river.data\n",
    "from river.data.datagenerator import DataGeneratorBilbyFD\n",
    "from river.data.dataset_multiband import DatasetMBStrainFDFromMBWFonGPU, DatasetMBStrainFDFromMBWFonGPUBatch\n",
    "#import river.data.utils as datautils\n",
    "from river.data.utils import *\n",
    "\n",
    "from river.models import embedding\n",
    "from river.models.utils import *\n",
    "from river.models.embedding.conv import EmbeddingConv1D, EmbeddingConv2D\n",
    "from river.models.embedding.mlp import EmbeddingMLP1D\n",
    "from river.models.inference.cnf import GlasNSFConv1DRes, GlasNSFConv1D, GlasNSFTest, GlasflowEmbdding\n",
    "\n",
    "import logging\n",
    "import sys\n",
    "import os\n",
    "import json\n",
    "from copy import deepcopy\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "bottom-motel",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "little-morgan",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22.30916714668274\n"
     ]
    }
   ],
   "source": [
    "t1 =time.time()\n",
    "dataset_train = DatasetMBStrainFDFromMBWFonGPUBatch(wf_folder = config_precaldata['train']['folder'],\n",
    "                                                    asd_folder = asd_folder,\n",
    "                                                    parameter_names = PARAMETER_NAMES_CONTEXT_PRECESSINGBNS_BILBY, \n",
    "                                                    full_duration = full_duration, \n",
    "                                                    detector_names = detector_names,\n",
    "                                                    dmin = dmin,\n",
    "                                                    dmax = dmax,\n",
    "                                                    dpower = dpower, \n",
    "                                                    tc_min = tc_min,\n",
    "                                                    tc_max = tc_max,\n",
    "                                                    timing_std = timing_std,\n",
    "                                                    device = device,\n",
    "                                                    minibatch_size = minibatch_size_train,\n",
    "                                                    add_noise = True,\n",
    "                                                    fix_extrinsic = False,\n",
    "                                                    reparameterize = True,\n",
    "                                                    random_asd = False)\n",
    "t2 =time.time()\n",
    "print(t2-t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "periodic-holder",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "config_path = '/home/qian.hu/mlpe/river/scripts/trained_models/BNS20MB_8M'\n",
    "with open(f\"{config_path}/config.json\", 'r') as f:\n",
    "    config = json.load(f)\n",
    "\n",
    "config_datagenerator = config['data_generator_parameters']\n",
    "config_training = config['training_parameters']\n",
    "config_model = config['model_parameters']\n",
    "config_precaldata = config['precaldata_parameters']\n",
    "\n",
    "\n",
    "dmin = config_datagenerator['d_min']\n",
    "dmax = config_datagenerator['d_max']\n",
    "dpower = config_datagenerator['d_power']\n",
    "tc_min = config_datagenerator['tc_min']\n",
    "tc_max = config_datagenerator['tc_max']\n",
    "timing_std = config_datagenerator['timing_std']\n",
    "full_duration = config_datagenerator['full_duration']\n",
    "\n",
    "\n",
    "\n",
    "# Set up logger\n",
    "PID = os.getpid()\n",
    "device='cuda'\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "formatter = logging.Formatter('%(asctime)s | %(levelname)s | %(message)s')\n",
    "\n",
    "stdout_handler = logging.StreamHandler(sys.stdout)\n",
    "stdout_handler.setLevel(logging.DEBUG)\n",
    "stdout_handler.setFormatter(formatter)\n",
    "\n",
    "ckpt_dir = 'test_train_output'\n",
    "if not os.path.exists(ckpt_dir):\n",
    "    os.mkdir(ckpt_dir)\n",
    "    logger.warning(f\"{ckpt_dir} does not exist. Made dir {ckpt_dir}.\")\n",
    "\n",
    "logfilename = f\"{ckpt_dir}/logs.log\"\n",
    "file_handler = logging.FileHandler(logfilename)\n",
    "file_handler.setLevel(logging.DEBUG)\n",
    "file_handler.setFormatter(formatter)\n",
    "logger.addHandler(file_handler)\n",
    "ckpt_path = f'{ckpt_dir}/checkpoint.pickle'\n",
    "\n",
    "logger.info(f'PID={PID}.')\n",
    "logger.info(f'Output path: {ckpt_dir}')\n",
    "\n",
    "detector_names = config_datagenerator['detector_names']\n",
    "\n",
    "\n",
    "logger.info(f'Loading precalculated data.')\n",
    "wf_folder_train = config_precaldata['valid']['folder']\n",
    "wf_folder_valid = config_precaldata['valid']['folder']\n",
    "asd_folder = config_precaldata['asd_path']\n",
    "\n",
    "batch_size_train = config_training['batch_size_valid']\n",
    "minibatch_size_train = config_training['minibatch_size_valid']\n",
    "batch_size_valid = config_training['batch_size_valid']\n",
    "minibatch_size_valid = config_training['minibatch_size_valid']\n",
    "\n",
    "\n",
    "dataset_train = DatasetMBStrainFDFromMBWFonGPUBatch(wf_folder = wf_folder_train,\n",
    "                                                    asd_folder = asd_folder,\n",
    "                                                    parameter_names = PARAMETER_NAMES_CONTEXT_PRECESSINGBNS_BILBY, \n",
    "                                                    full_duration = full_duration, \n",
    "                                                    detector_names = detector_names,\n",
    "                                                    dmin = dmin,\n",
    "                                                    dmax = dmax,\n",
    "                                                    dpower = dpower, \n",
    "                                                    tc_min = tc_min,\n",
    "                                                    tc_max = tc_max,\n",
    "                                                    timing_std = timing_std,\n",
    "                                                    device = device,\n",
    "                                                    minibatch_size = minibatch_size_train,\n",
    "                                                    add_noise = True,\n",
    "                                                    fix_extrinsic = False,\n",
    "                                                    reparameterize = True,\n",
    "                                                    random_asd = False)\n",
    "\n",
    "dataset_valid = DatasetMBStrainFDFromMBWFonGPUBatch(wf_folder = wf_folder_valid,\n",
    "                                                    asd_folder = asd_folder,\n",
    "                                                    parameter_names = PARAMETER_NAMES_CONTEXT_PRECESSINGBNS_BILBY, \n",
    "                                                    full_duration = full_duration, \n",
    "                                                    detector_names = detector_names,\n",
    "                                                    dmin = dmin,\n",
    "                                                    dmax = dmax,\n",
    "                                                    dpower = dpower, \n",
    "                                                    tc_min = tc_min,\n",
    "                                                    tc_max = tc_max,\n",
    "                                                    timing_std = timing_std,\n",
    "                                                    device = device,\n",
    "                                                    minibatch_size = minibatch_size_train,\n",
    "                                                    add_noise = True,\n",
    "                                                    fix_extrinsic = False,\n",
    "                                                    reparameterize = True,\n",
    "                                                    random_asd = False)\n",
    "\n",
    "\n",
    "Nsample = len(dataset_train)*minibatch_size_train\n",
    "Nvalid = len(dataset_valid)*minibatch_size_valid\n",
    "logger.info(f'Nsample: {Nsample}, Nvalid: {Nvalid}.')\n",
    "logger.info(f'batch_size_train: {batch_size_train}, batch_size_valid: {batch_size_valid}')\n",
    "\n",
    "train_loader = DataLoader(dataset_train, batch_size=batch_size_train // minibatch_size_train, shuffle=False)\n",
    "valid_loader = DataLoader(dataset_valid, batch_size=batch_size_valid // minibatch_size_valid, shuffle=False)\n",
    "\n",
    "model = GlasflowEmbdding(config).to(device)\n",
    "\n",
    "\n",
    "lr = config_training['lr']\n",
    "gamma = config_training['gamma']\n",
    "weight_decay = config_training['weight_decay']\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "logger.info(f'Initial learning rate: {lr}')\n",
    "logger.info(f'Gamma: {gamma}')\n",
    "\n",
    "max_epoch = config_training['max_epoch']\n",
    "#epoches_pretrain = config_training['epoches_pretrain']\n",
    "epoches_save_loss = config_training['epoches_save_loss']\n",
    "epoches_adjust_lr = config_training['epoches_adjust_lr']\n",
    "epoches_adjust_lr_again = config_training['epoches_adjust_lr_again']\n",
    "#load_from_previous_train = 1\n",
    "load_from_previous_train = config_training['load_from_previous_train']\n",
    "if load_from_previous_train:\n",
    "    checkpoint = torch.load(ckpt_path)\n",
    "\n",
    "    best_epoch = checkpoint['epoch']\n",
    "    start_epoch = best_epoch + 1\n",
    "    lr_updated_epoch = start_epoch\n",
    "    model.load_state_dict(checkpoint['model_state_dict']) \n",
    "\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "\n",
    "\n",
    "    train_losses = checkpoint['train_losses']\n",
    "    valid_losses = checkpoint['valid_losses']\n",
    "\n",
    "\n",
    "    logger.info(f'Loaded states from {ckpt_path}, epoch={start_epoch}.')\n",
    "else:\n",
    "    best_epoch = 0\n",
    "    train_losses = []\n",
    "    valid_losses = []\n",
    "\n",
    "    start_epoch = 0\n",
    "    lr_updated_epoch = start_epoch\n",
    "\n",
    "npara_embd = count_parameters(model.embedding)\n",
    "npara_flow = count_parameters(model.flow)\n",
    "npara_total = count_parameters(model)\n",
    "logger.info(f'Learnable parameters: embedding: {npara_embd}, flow: {npara_flow}, total: {npara_total}. ')\n",
    "\n",
    "###\n",
    "#for g in optimizer.param_groups:\n",
    "#    g['lr'] = 1e-5\n",
    "#    logger.info(f'Set lr to 1e-5.')\n",
    "\n",
    "logger.info(f'Training started, device:{device}. ')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "applicable-nancy",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start_epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "accompanied-heather",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "speaking-aquatic",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for epoch in range(start_epoch, 1):    \n",
    "\n",
    "    train_loss, train_loss_std = train_GlasNSFWarpper(model, optimizer, train_loader, device=device, minibatch_size=minibatch_size_train)\n",
    "    valid_loss, valid_loss_std = eval_GlasNSFWarpper(model, valid_loader, device=device, minibatch_size=minibatch_size_valid)\n",
    "\n",
    "\n",
    "    train_losses.append(train_loss)\n",
    "    valid_losses.append(valid_loss)\n",
    "\n",
    "    logger.info(f'epoch {epoch}, train loss = {train_loss}±{train_loss_std}, valid loss = {valid_loss}±{valid_loss_std}')\n",
    "\n",
    "    if valid_loss==min(valid_losses):\n",
    "        best_epoch = epoch\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'train_losses': train_losses,\n",
    "            'valid_losses': valid_losses,\n",
    "            }, ckpt_path)\n",
    "\n",
    "        logger.info(f'Current best epoch: {best_epoch}. Checkpoint saved.')\n",
    "\n",
    "    if epoch%epoches_save_loss == 0 and epoch!=0:\n",
    "        save_loss_data(train_losses, valid_losses, ckpt_dir)\n",
    "\n",
    "    if epoch-best_epoch>=epoches_adjust_lr and epoch-lr_updated_epoch>=epoches_adjust_lr_again:\n",
    "        adjust_lr(optimizer, gamma)\n",
    "        logger.info(f'Validation loss has not dropped for {epoch-best_epoch} epoches. Learning rate is decreased by a factor of {gamma}.')\n",
    "        lr_updated_epoch = epoch\n",
    "\n",
    "    #dataset_train.shuffle_indexinfile()\n",
    "    dataset_train.shuffle_wflist()\n",
    "    train_loader = DataLoader(dataset_train, batch_size=batch_size_train // minibatch_size_train, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "sunrise-cooler",
   "metadata": {},
   "outputs": [],
   "source": [
    "aa = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "fundamental-grenada",
   "metadata": {},
   "outputs": [],
   "source": [
    "aa.append(None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "dying-billy",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[None]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exciting-freeware",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "awful-yahoo",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23.024776458740234"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "historic-tracker",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7709.6376953125"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "continental-inside",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "134071248.0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_loss_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cardiac-information",
   "metadata": {},
   "outputs": [],
   "source": [
    "for t, x in train_loader:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "competent-trustee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([50, 10, 17])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "continuing-robert",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([50, 10, 6, 3328])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "domestic-journalism",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_list = []\n",
    "minibatch_size = 10\n",
    "for theta, x in train_loader:\n",
    "    theta = theta.to(device)\n",
    "    x = x.to(device)\n",
    "\n",
    "    if minibatch_size>0:\n",
    "        # x: [bs, minibatch_size, nchannel, nbasis]\n",
    "        # theta: [bs, minibatch_size, npara]\n",
    "        bs = x.shape[0]\n",
    "        nbasis = x.shape[-1]\n",
    "        nchannel = x.shape[-2]\n",
    "        npara = theta.shape[-1]\n",
    "        theta = theta.view(bs*minibatch_size, npara)\n",
    "        x = x.view(bs*minibatch_size, nchannel, nbasis)\n",
    "    loss = -model.log_prob(theta, x).mean()\n",
    "\n",
    "    loss_list.append(loss.detach())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "damaged-leader",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor(7733.1514, device='cuda:0'), tensor(7739.5537, device='cuda:0')]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "interested-simulation",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([500, 6, 3328])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "christian-jewel",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(7739.5537, device='cuda:0', grad_fn=<NegBackward0>)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "complimentary-going",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
